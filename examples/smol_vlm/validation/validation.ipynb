{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98ce996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd567e71",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://artwyrd.com/wp-content/uploads/2021/01/angela-porter-2021-jan-25.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# image1 = load_image(\"https://media.istockphoto.com/id/485371557/photo/twilight-at-spirit-island.jpg?s=612x612&w=0&k=20&c=FSGliJ4EKFP70Yjpzso0HfRR4WwflC6GKfl4F3Hj7fk=\")\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# image2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# image1 = Image.open(\"../../../.vscode/fuji-mountain-in-autumn.jpg\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m image1 = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://artwyrd.com/wp-content/uploads/2021/01/angela-porter-2021-jan-25.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage 1 size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage1.size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize processor and model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/PIL/Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'https://artwyrd.com/wp-content/uploads/2021/01/angela-porter-2021-jan-25.jpg'"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "# image1 = load_image(\"https://media.istockphoto.com/id/485371557/photo/twilight-at-spirit-island.jpg?s=612x612&w=0&k=20&c=FSGliJ4EKFP70Yjpzso0HfRR4WwflC6GKfl4F3Hj7fk=\")\n",
    "# image2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\n",
    "# image1 = Image.open(\"../../../.vscode/fuji-mountain-in-autumn.jpg\")\n",
    "image1 = load_image(\"https://artwyrd.com/wp-content/uploads/2021/01/angela-porter-2021-jan-25.jpg\")\n",
    "print(f\"Image 1 size: {image1.size}\")\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "\n",
    "# Create input messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe the image?\"}\n",
    "            # {\"type\": \"text\", \"text\": \"A real-valued function f defined on the real line is called an even function if f(-t) = f(t) for each real number t. Prove that the set of even functions defined on the real line with the operations of addition and scalar multiplication defined in Example 3 is a vector space.\"}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67fa3593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.idefics3.modeling_idefics3.Idefics3ForConditionalGeneration"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model directly on CUDA without Flash Attention\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # _attn_implementation=\"flash_attention_2\",  # Commented out Flash Attention\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0599d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 11126,    42,  ...,  9519,  9531,    42]], device='cuda:0')\n",
      ">> User:<image>Can you describe the image?\n",
      "Assistant: The image features a scenic view of a mountain, likely Mount Fuji in Japan, with a bridge in the foreground and a body of water in the foreground. The mountain is partially covered with snow, indicating it is likely in the winter season. The sky is clear with a few clouds, and the water in the foreground is calm and still, reflecting the colors of the sky and the mountain. \n",
      "\n",
      "The bridge is a long, narrow structure that spans the body of water, connecting the land on both sides. The bridge is made of a light-colored material, possibly wood or concrete, and it appears to be sturdy and well-maintained. The bridge is surrounded by trees, which are mostly green but some have hints of autumn colors, suggesting the season is changing.\n",
      "\n",
      "The water in the foreground is calm and still, with a few ripples on the surface, reflecting the colors of the sky and the mountain. The water is a deep blue color, which contrasts with the light blue sky. The sky is clear with a few clouds, and the sun is not visible in the image, suggesting it is either early morning or late evening.\n",
      "\n",
      "The mountain is the main focus of the image, and it is partially covered with snow. The snow is white and pristine, indicating it is likely a cold day. The mountain is tall and majestic, with a symmetrical shape and a pointed peak. The mountain is surrounded by trees, which are mostly green but some have hints of autumn colors, suggesting the season is changing.\n",
      "\n",
      "The image is peaceful and serene, capturing the beauty of nature and the tranquility of the scene. The use of light and color in the image creates a sense of depth and atmosphere, enhancing the overall aesthetic appeal.\n",
      "\n",
      "### Analysis:\n",
      "\n",
      "The image is a beautiful depiction of a mountain landscape, capturing the essence of a serene winter day. The use of light and color in the image creates a sense of depth and atmosphere, enhancing the overall aesthetic appeal. The mountain is the main focus, with its snow-covered peak and symmetrical shape, creating a sense of grandeur and tranquility. The bridge in the foreground adds a sense of human connection to the scene, while the trees and water in the foreground add a touch of natural beauty. The clear sky and calm water further enhance the peaceful and serene atmosphere of the image.\n",
      "\n",
      "### Conclusion:\n",
      "\n",
      "The image is a beautiful depiction of a mountain landscape, capturing the essence of a serene winter day.\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
    "# inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "print(inputs[\"input_ids\"])\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,  # Use greedy decoding (highest logit)\n",
    ")\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,  # Slice to include only the first token\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(\">>\", generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenP = [    1, 11126,    42,   330,  1345,    29, 34270,  1517,   275,  4355,\n",
    "           335,   260,  1345,  1761,   314,  1217,   354,   908,  1517,   585,\n",
    "           275, 10242,   100,    25,   446,   275,    24,   100,    25,   327,\n",
    "           971,  1345,  1230,   252,    30,  1053,   307,   338,   260,   932,\n",
    "           282,   908,  3691,  4355,   335,   260,  1345,  1761,   351,   260,\n",
    "          4261,   282,  1706,   284, 26727, 17385,  4355,   281, 12066,   216,\n",
    "            35,   314,   253,  8431,  1898,    30, 49154,   198,  9519,  9531,\n",
    "            42]\n",
    "tokenR = [1, 11126, 42, 330, 1345, 29, 34270, 1517, 275, 4355, 335, 260, 1345, 1761, 314, 1217, 354, 908, 1517, 585, 275, 10242, 100, 25, 446, 275, 24, 100, 25, 327, 971, 1345, 1230, 252, 30, 1053, 307, 338, 260, 932, 282, 908, 3691, 4355, 335, 260, 1345, 1761, 351, 260, 4261, 282, 1706, 284, 26727, 17385, 4355, 281, 12066, 216, 35, 314, 253, 8431, 1898, 30, 49154, 198, 9519, 9531, 42]\n",
    "\n",
    "print(\"Token same?\", tokenP == tokenR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd57e3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\n",
      "{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.chat_template)\n",
    "\"\"\"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "<|im_start|>\n",
    "{% for message in messages %}\n",
    "    {{message['role'] | capitalize}}\n",
    "    {% if message['content'][0]['type'] == 'image' %}\n",
    "        {{':'}}\n",
    "    {% else %}\n",
    "        {{': '}}\n",
    "    {% endif %}\n",
    "    {% for line in message['content'] %}\n",
    "        {% if line['type'] == 'text' %}\n",
    "            {{line['text']}}\n",
    "        {% elif line['type'] == 'image' %}\n",
    "            {{ '<image>' }}\n",
    "        {% endif %}\n",
    "    {% endfor %}\n",
    "    <end_of_utterance>]\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "    {{ 'Assistant:' }}\n",
    "{% endif %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e88a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 MODEL ARCHITECTURE VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "📋 Model Information:\n",
      "  Model ID: HuggingFaceTB/SmolVLM-Instruct\n",
      "  Model Class: Idefics3ForConditionalGeneration\n",
      "  Model Module: transformers.models.idefics3.modeling_idefics3\n",
      "  Processor Class: Idefics3Processor\n",
      "\n",
      "⚙️  Model Configuration:\n",
      "  Model Type: idefics3\n",
      "  Architecture: ['Idefics3ForConditionalGeneration']\n",
      "  Vision Config: Idefics3VisionConfig\n",
      "  Text Config: LlamaConfig\n",
      "\n",
      "🤔 Is this really SmolVLM?\n",
      "  Config says model_type: idefics3\n",
      "  Class name suggests: Idefics3\n",
      "\n",
      "📊 Model Size:\n",
      "  Total parameters: 2,246,272,880\n",
      "  Trainable parameters: 2,246,272,880\n",
      "  Model size: ~2.2B parameters\n",
      "\n",
      "🔧 Key Config Details:\n",
      "  Text model: llama\n",
      "  Vocab size: 49155\n",
      "  Hidden size: 2048\n",
      "  Vision model: idefics3\n",
      "  Image size: 384\n",
      "  Patch size: 14\n",
      "\n",
      "💡 CONCLUSION:\n",
      "  SmolVLM appears to be BASED ON Idefics3 architecture!\n",
      "  This is why you're seeing Idefics3 classes everywhere.\n",
      "  SmolVLM is likely a fine-tuned or modified version of Idefics3.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 COMPREHENSIVE MODEL VERIFICATION\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 MODEL ARCHITECTURE VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check model ID and name\n",
    "print(f\"\\n📋 Model Information:\")\n",
    "print(f\"  Model ID: HuggingFaceTB/SmolVLM-Instruct\")\n",
    "print(f\"  Model Class: {type(model).__name__}\")\n",
    "print(f\"  Model Module: {type(model).__module__}\")\n",
    "print(f\"  Processor Class: {type(processor).__name__}\")\n",
    "\n",
    "# Check model config\n",
    "print(f\"\\n⚙️  Model Configuration:\")\n",
    "config = model.config\n",
    "print(f\"  Model Type: {getattr(config, 'model_type', 'Unknown')}\")\n",
    "print(f\"  Architecture: {getattr(config, 'architectures', 'Unknown')}\")\n",
    "print(f\"  Vision Config: {type(getattr(config, 'vision_config', None)).__name__ if hasattr(config, 'vision_config') else 'None'}\")\n",
    "print(f\"  Text Config: {type(getattr(config, 'text_config', None)).__name__ if hasattr(config, 'text_config') else 'None'}\")\n",
    "\n",
    "# Check if it's really SmolVLM or Idefics3\n",
    "print(f\"\\n🤔 Is this really SmolVLM?\")\n",
    "print(f\"  Config says model_type: {getattr(config, 'model_type', 'Unknown')}\")\n",
    "print(f\"  Class name suggests: {'Idefics3' if 'idefics3' in type(model).__name__.lower() else 'Other'}\")\n",
    "\n",
    "# Check model parameters\n",
    "print(f\"\\n📊 Model Size:\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params / 1e9:.1f}B parameters\")\n",
    "\n",
    "# Check config details\n",
    "print(f\"\\n🔧 Key Config Details:\")\n",
    "if hasattr(config, 'text_config'):\n",
    "    text_config = config.text_config\n",
    "    print(f\"  Text model: {getattr(text_config, 'model_type', 'Unknown')}\")\n",
    "    print(f\"  Vocab size: {getattr(text_config, 'vocab_size', 'Unknown')}\")\n",
    "    print(f\"  Hidden size: {getattr(text_config, 'hidden_size', 'Unknown')}\")\n",
    "\n",
    "if hasattr(config, 'vision_config'):\n",
    "    vision_config = config.vision_config\n",
    "    print(f\"  Vision model: {getattr(vision_config, 'model_type', 'Unknown')}\")\n",
    "    print(f\"  Image size: {getattr(vision_config, 'image_size', 'Unknown')}\")\n",
    "    print(f\"  Patch size: {getattr(vision_config, 'patch_size', 'Unknown')}\")\n",
    "\n",
    "# The revelation\n",
    "print(f\"\\n💡 CONCLUSION:\")\n",
    "print(f\"  SmolVLM appears to be BASED ON Idefics3 architecture!\")\n",
    "print(f\"  This is why you're seeing Idefics3 classes everywhere.\")\n",
    "print(f\"  SmolVLM is likely a fine-tuned or modified version of Idefics3.\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "550bbec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 SEARCHING FOR DEDICATED SMOLVLM CLASSES\n",
      "================================================================================\n",
      "\n",
      "📦 Transformers version: 4.53.1\n",
      "\n",
      "🔍 Looking for SmolVLM classes...\n",
      "✅ SmolVLMForConditionalGeneration - FOUND!\n",
      "✅ SmolVLMProcessor - FOUND!\n",
      "✅ SmolVLMImageProcessor - FOUND!\n",
      "\n",
      "📋 Available model types in transformers:\n",
      "  SmolVLM-related modules: ['SmolLM3Config', 'SmolLM3ForCausalLM', 'SmolLM3ForQuestionAnswering', 'SmolLM3ForSequenceClassification', 'SmolLM3ForTokenClassification', 'SmolLM3Model', 'SmolLM3PreTrainedModel', 'SmolVLMConfig', 'SmolVLMForConditionalGeneration', 'SmolVLMImageProcessor', 'SmolVLMImageProcessorFast', 'SmolVLMModel', 'SmolVLMPreTrainedModel', 'SmolVLMProcessor', 'SmolVLMVideoProcessor', 'SmolVLMVisionConfig', 'SmolVLMVisionTransformer', 'smollm3', 'smollm3.configuration_smollm3', 'smollm3.modeling_smollm3', 'smolvlm', 'smolvlm.configuration_smolvlm', 'smolvlm.image_processing_smolvlm', 'smolvlm.image_processing_smolvlm_fast', 'smolvlm.modeling_smolvlm', 'smolvlm.processing_smolvlm', 'smolvlm.video_processing_smolvlm']\n",
      "\n",
      "🗺️  Model mapping for SmolVLM:\n",
      "  Config type: Idefics3Config\n",
      "  Model type from config: idefics3\n",
      "  Model type 'idefics3' not in vision mapping\n",
      "\n",
      "💡 RECOMMENDATION:\n",
      "  Use dedicated SmolVLM classes if available!\n",
      "  Try: from transformers import SmolVLMForConditionalGeneration, SmolVLMProcessor\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 CHECK FOR DEDICATED SMOLVLM CLASSES\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 SEARCHING FOR DEDICATED SMOLVLM CLASSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import transformers\n",
    "print(f\"\\n📦 Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Check if SmolVLM classes exist\n",
    "print(f\"\\n🔍 Looking for SmolVLM classes...\")\n",
    "\n",
    "try:\n",
    "    from transformers import SmolVLMForConditionalGeneration\n",
    "    print(\"✅ SmolVLMForConditionalGeneration - FOUND!\")\n",
    "except ImportError:\n",
    "    print(\"❌ SmolVLMForConditionalGeneration - NOT FOUND\")\n",
    "\n",
    "try:\n",
    "    from transformers import SmolVLMProcessor\n",
    "    print(\"✅ SmolVLMProcessor - FOUND!\")\n",
    "except ImportError:\n",
    "    print(\"❌ SmolVLMProcessor - NOT FOUND\")\n",
    "\n",
    "try:\n",
    "    from transformers import SmolVLMImageProcessor  \n",
    "    print(\"✅ SmolVLMImageProcessor - FOUND!\")\n",
    "except ImportError:\n",
    "    print(\"❌ SmolVLMImageProcessor - NOT FOUND\")\n",
    "\n",
    "# Check what's available in transformers.models\n",
    "print(f\"\\n📋 Available model types in transformers:\")\n",
    "import transformers.models\n",
    "model_modules = [name for name in dir(transformers.models) if not name.startswith('_')]\n",
    "smol_modules = [name for name in model_modules if 'smol' in name.lower()]\n",
    "print(f\"  SmolVLM-related modules: {smol_modules}\")\n",
    "\n",
    "# Check model mapping\n",
    "print(f\"\\n🗺️  Model mapping for SmolVLM:\")\n",
    "try:\n",
    "    from transformers import MODEL_FOR_VISION_2_SEQ_MAPPING, AutoConfig\n",
    "    \n",
    "    # Try to get config for SmolVLM\n",
    "    config = AutoConfig.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "    print(f\"  Config type: {type(config).__name__}\")\n",
    "    print(f\"  Model type from config: {getattr(config, 'model_type', 'Unknown')}\")\n",
    "    \n",
    "    # Check what model class this maps to\n",
    "    if hasattr(config, 'model_type'):\n",
    "        model_type = config.model_type\n",
    "        if model_type in MODEL_FOR_VISION_2_SEQ_MAPPING:\n",
    "            mapped_class = MODEL_FOR_VISION_2_SEQ_MAPPING[model_type]\n",
    "            print(f\"  Maps to class: {mapped_class.__name__}\")\n",
    "        else:\n",
    "            print(f\"  Model type '{model_type}' not in vision mapping\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"  Error checking mapping: {e}\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATION:\")\n",
    "if smol_modules:\n",
    "    print(f\"  Use dedicated SmolVLM classes if available!\")\n",
    "    print(f\"  Try: from transformers import SmolVLMForConditionalGeneration, SmolVLMProcessor\")\n",
    "else:\n",
    "    print(f\"  No dedicated SmolVLM classes found.\")\n",
    "    print(f\"  SmolVLM likely uses Idefics3 architecture as base.\")\n",
    "    print(f\"  Consider updating transformers: pip install --upgrade transformers\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3bb3ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TESTING DEDICATED SMOLVLM CLASSES\n",
      "================================================================================\n",
      "🔄 Attempting to load with SmolVLMForConditionalGeneration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ SmolVLM classes not available: Package `num2words` is required to run SmolVLM processor. Install it with `pip install num2words`.\n",
      "💡 This confirms SmolVLM uses Idefics3 architecture\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🧪 TEST LOADING WITH DEDICATED SMOLVLM CLASSES\n",
    "print(\"=\" * 80)\n",
    "print(\"🧪 TESTING DEDICATED SMOLVLM CLASSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Try to load with dedicated SmolVLM classes\n",
    "try:\n",
    "    print(\"🔄 Attempting to load with SmolVLMForConditionalGeneration...\")\n",
    "    from transformers import SmolVLMForConditionalGeneration, SmolVLMProcessor\n",
    "    \n",
    "    # Load with dedicated classes\n",
    "    smol_processor = SmolVLMProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "    smol_model = SmolVLMForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    \n",
    "    print(\"✅ SUCCESS! Loaded with dedicated SmolVLM classes!\")\n",
    "    print(f\"  Model type: {type(smol_model).__name__}\")\n",
    "    print(f\"  Processor type: {type(smol_processor).__name__}\")\n",
    "    print(f\"  Model module: {type(smol_model).__module__}\")\n",
    "    \n",
    "    # Compare with current model\n",
    "    print(f\"\\n🔍 Comparison with current model:\")\n",
    "    print(f\"  Current model: {type(model).__name__}\")\n",
    "    print(f\"  New model: {type(smol_model).__name__}\")\n",
    "    print(f\"  Same class? {type(model) == type(smol_model)}\")\n",
    "    \n",
    "    # Check if preprocessing differs\n",
    "    print(f\"\\n🔍 Processor comparison:\")\n",
    "    print(f\"  Current processor: {type(processor).__name__}\")\n",
    "    print(f\"  New processor: {type(smol_processor).__name__}\")\n",
    "    print(f\"  Same class? {type(processor) == type(smol_processor)}\")\n",
    "    \n",
    "    # Update globals if different\n",
    "    if type(model) != type(smol_model):\n",
    "        print(f\"\\n🔄 Updating to use dedicated SmolVLM classes...\")\n",
    "        globals()['model'] = smol_model\n",
    "        globals()['processor'] = smol_processor\n",
    "        print(\"✅ Updated! Now using dedicated SmolVLM classes.\")\n",
    "    else:\n",
    "        print(f\"\\n💡 Same classes - no difference between Auto and dedicated imports.\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ SmolVLM classes not available: {e}\")\n",
    "    print(\"💡 This confirms SmolVLM uses Idefics3 architecture\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading with SmolVLM classes: {e}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb1a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 ANALYZING CAUSAL MASKING IN SMOLVLM\n",
      "================================================================================\n",
      "\n",
      "⚙️  Model Configuration Analysis:\n",
      "  Model type: idefics3\n",
      "\n",
      "📝 Text Model Configuration:\n",
      "  Text model type: llama\n",
      "  Is decoder: False\n",
      "  Is encoder decoder: False\n",
      "  Add cross attention: False\n",
      "\n",
      "🔍 Model Architecture Analysis:\n",
      "  Model class: Idefics3ForConditionalGeneration\n",
      "\n",
      "🏗️  Architecture Analysis:\n",
      "  🤔 This is a vision-language model - causal masking depends on implementation\n",
      "\n",
      "🧪 Testing Generation Behavior:\n",
      "  Input: 'The quick brown'\n",
      "  Generated: 'The quick brown fox jumped over the lazy'\n",
      "  → Looks like causal generation!\n",
      "\n",
      "💡 CONCLUSION:\n",
      "  SmolVLM appears to be a vision-language model that:\n",
      "  - Uses causal attention for text generation (like GPT)\n",
      "  - Processes images with bidirectional attention\n",
      "  - Combines both modalities for conditional generation\n",
      "  - Likely implements causal masking in the text decoder part\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 CHECK FOR CAUSAL MASKING IN SMOLVLM\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 ANALYZING CAUSAL MASKING IN SMOLVLM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check model configuration for causal attention\n",
    "config = model.config\n",
    "print(f\"\\n⚙️  Model Configuration Analysis:\")\n",
    "print(f\"  Model type: {getattr(config, 'model_type', 'Unknown')}\")\n",
    "\n",
    "# Check text model configuration (where causal masking would be)\n",
    "if hasattr(config, 'text_config'):\n",
    "    text_config = config.text_config\n",
    "    print(f\"\\n📝 Text Model Configuration:\")\n",
    "    print(f\"  Text model type: {getattr(text_config, 'model_type', 'Unknown')}\")\n",
    "    print(f\"  Is decoder: {getattr(text_config, 'is_decoder', 'Unknown')}\")\n",
    "    print(f\"  Is encoder decoder: {getattr(text_config, 'is_encoder_decoder', 'Unknown')}\")\n",
    "    print(f\"  Add cross attention: {getattr(text_config, 'add_cross_attention', 'Unknown')}\")\n",
    "    \n",
    "    # Check for causal attention settings\n",
    "    causal_attrs = ['is_causal', 'causal', 'use_causal_mask', 'causal_attention']\n",
    "    for attr in causal_attrs:\n",
    "        if hasattr(text_config, attr):\n",
    "            print(f\"  {attr}: {getattr(text_config, attr)}\")\n",
    "\n",
    "# Inspect the actual model layers for attention patterns\n",
    "print(f\"\\n🔍 Model Architecture Analysis:\")\n",
    "print(f\"  Model class: {type(model).__name__}\")\n",
    "\n",
    "# Look at the text model specifically\n",
    "if hasattr(model, 'text_model'):\n",
    "    text_model = model.text_model\n",
    "    print(f\"  Text model class: {type(text_model).__name__}\")\n",
    "    \n",
    "    # Check if it has layers with attention\n",
    "    if hasattr(text_model, 'layers'):\n",
    "        print(f\"  Number of layers: {len(text_model.layers)}\")\n",
    "        \n",
    "        # Inspect first layer's attention\n",
    "        if len(text_model.layers) > 0:\n",
    "            first_layer = text_model.layers[0]\n",
    "            print(f\"  First layer class: {type(first_layer).__name__}\")\n",
    "            \n",
    "            if hasattr(first_layer, 'self_attn'):\n",
    "                attn = first_layer.self_attn\n",
    "                print(f\"  Attention class: {type(attn).__name__}\")\n",
    "                \n",
    "                # Check for causal mask attributes\n",
    "                causal_attrs = ['is_causal', 'causal', 'use_causal_mask']\n",
    "                for attr in causal_attrs:\n",
    "                    if hasattr(attn, attr):\n",
    "                        print(f\"  Attention {attr}: {getattr(attn, attr)}\")\n",
    "\n",
    "# Check what type of model this is based on architecture\n",
    "print(f\"\\n🏗️  Architecture Analysis:\")\n",
    "model_name = type(model).__name__.lower()\n",
    "if 'llama' in model_name or 'qwen' in model_name or 'phi' in model_name:\n",
    "    print(\"  ✅ This appears to be based on a decoder-only architecture (likely causal)\")\n",
    "elif 'bert' in model_name or 'encoder' in model_name:\n",
    "    print(\"  ❌ This appears to be based on an encoder architecture (likely not causal)\")\n",
    "elif 'idefics' in model_name or 'vlm' in model_name:\n",
    "    print(\"  🤔 This is a vision-language model - causal masking depends on implementation\")\n",
    "\n",
    "# Test with a simple generation to see behavior\n",
    "print(f\"\\n🧪 Testing Generation Behavior:\")\n",
    "test_prompt = \"The quick brown\"\n",
    "test_inputs = processor(text=test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate with temperature=0 for deterministic output\n",
    "    outputs = model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        pad_token_id=processor.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"  Input: '{test_prompt}'\")\n",
    "print(f\"  Generated: '{generated_text}'\")\n",
    "print(f\"  → {'Looks like causal generation!' if len(generated_text) > len(test_prompt) else 'No new tokens generated'}\")\n",
    "\n",
    "print(f\"\\n💡 CONCLUSION:\")\n",
    "print(f\"  SmolVLM appears to be a vision-language model that:\")\n",
    "print(f\"  - Uses causal attention for text generation (like GPT)\")\n",
    "print(f\"  - Processes images with bidirectional attention\")\n",
    "print(f\"  - Combines both modalities for conditional generation\")\n",
    "print(f\"  - Likely implements causal masking in the text decoder part\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc471df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔍 DEBUGGING RUST vs PYTHON PARITY ISSUES\n",
      "================================================================================\n",
      "🐍 PYTHON IMPLEMENTATION ANALYSIS:\n",
      "==================================================\n",
      "\n",
      "📊 Model Precision:\n",
      "  Model dtype: torch.bfloat16\n",
      "  Model device: cuda:0\n",
      "\n",
      "⚙️  Generation Configuration:\n",
      "  do_sample: False\n",
      "  temperature: 1.0\n",
      "  top_p: 1.0\n",
      "  top_k: 50\n",
      "  repetition_penalty: 1.0\n",
      "  length_penalty: 1.0\n",
      "  no_repeat_ngram_size: 0\n",
      "\n",
      "🎯 TESTING MAXIMUM DETERMINISM:\n",
      "========================================\n",
      "Input prompt: '<|im_start|>User: What color is the sky?<end_of_utterance>\n",
      "Assistant:'\n",
      "Input token IDs: [[1, 11126, 42, 1812, 2380, 314, 260, 6376, 47, 49154, 198, 9519, 9531, 42]]\n",
      "\n",
      "Generated tokens: [1, 11126, 42, 1812, 2380, 314, 260, 6376, 47, 49154, 198, 9519, 9531, 42, 10062, 30, 49154]\n",
      "Generated text: 'User: What color is the sky?\n",
      "Assistant: Blue.'\n",
      "\n",
      "New tokens only: [10062, 30, 49154]\n",
      "New text only: ' Blue.'\n",
      "\n",
      "🔧 POTENTIAL SOURCES OF DIFFERENCES:\n",
      "==================================================\n",
      "\n",
      "1. 🔢 NUMERICAL PRECISION:\n",
      "   - Python: Uses PyTorch's bf16/f32 precision\n",
      "   - Rust: Candle's precision might differ slightly\n",
      "   - Solution: Ensure exact same dtype operations\n",
      "\n",
      "2. 🧮 ATTENTION COMPUTATION:\n",
      "   - Softmax numerical stability differences\n",
      "   - Matrix multiplication order/accumulation\n",
      "   - Causal mask application timing\n",
      "   - Solution: Match exact computation order\n",
      "\n",
      "3. 🎭 LAYER NORMALIZATION:\n",
      "   - Epsilon values (1e-5 vs 1e-6)\n",
      "   - Computation order (x-mean)/std vs x/std - mean/std\n",
      "   - Solution: Use exact same epsilon and formula\n",
      "\n",
      "4. 🔄 TOKENIZATION:\n",
      "   - Preprocessing differences\n",
      "   - Special token handling\n",
      "   - Solution: Use identical tokenizer and special tokens\n",
      "\n",
      "5. 🖼️  IMAGE PREPROCESSING:\n",
      "   - Resize/normalization differences\n",
      "   - Padding strategies\n",
      "   - Patch splitting logic\n",
      "   - Solution: Match exact preprocessing pipeline\n",
      "\n",
      "6. 💾 WEIGHT LOADING:\n",
      "   - Tensor layout differences (row vs column major)\n",
      "   - Weight precision during loading\n",
      "   - Solution: Verify weight values match exactly\n",
      "\n",
      "7. 🎲 GENERATION LOGIC:\n",
      "   - Logit processing order\n",
      "   - Temperature application\n",
      "   - Token selection tie-breaking\n",
      "   - Solution: Match exact generation pipeline\n",
      "\n",
      "\n",
      "✅ ACHIEVING EXACT PARITY:\n",
      "==============================\n",
      "\n",
      "To get exact parity:\n",
      "\n",
      "1. 🔍 INTERMEDIATE OUTPUT COMPARISON:\n",
      "   - Print logits before softmax in both implementations\n",
      "   - Compare attention weights layer by layer\n",
      "   - Verify embeddings and hidden states match\n",
      "\n",
      "2. 🧪 UNIT TESTING:\n",
      "   - Test individual components (attention, MLP, etc.)\n",
      "   - Use simple inputs (zeros, ones, known patterns)\n",
      "   - Compare outputs at each layer\n",
      "\n",
      "3. 🎯 WEIGHT VERIFICATION:\n",
      "   - Extract weights from Python model\n",
      "   - Verify they load identically in Rust\n",
      "   - Check for any transpose/reshape differences\n",
      "\n",
      "4. 📊 PRECISION CONTROL:\n",
      "   - Use identical dtypes throughout pipeline\n",
      "   - Implement identical numerical algorithms\n",
      "   - Handle edge cases (div by zero, etc.) the same way\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 ANALYZING RUST vs PYTHON IMPLEMENTATION DIFFERENCES\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 DEBUGGING RUST vs PYTHON PARITY ISSUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check current Python settings for maximum determinism\n",
    "print(\"🐍 PYTHON IMPLEMENTATION ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check model precision and settings\n",
    "print(f\"\\n📊 Model Precision:\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Check generation config\n",
    "gen_config = model.generation_config\n",
    "print(f\"\\n⚙️  Generation Configuration:\")\n",
    "print(f\"  do_sample: {getattr(gen_config, 'do_sample', 'Not set')}\")\n",
    "print(f\"  temperature: {getattr(gen_config, 'temperature', 'Not set')}\")\n",
    "print(f\"  top_p: {getattr(gen_config, 'top_p', 'Not set')}\")\n",
    "print(f\"  top_k: {getattr(gen_config, 'top_k', 'Not set')}\")\n",
    "print(f\"  repetition_penalty: {getattr(gen_config, 'repetition_penalty', 'Not set')}\")\n",
    "print(f\"  length_penalty: {getattr(gen_config, 'length_penalty', 'Not set')}\")\n",
    "print(f\"  no_repeat_ngram_size: {getattr(gen_config, 'no_repeat_ngram_size', 'Not set')}\")\n",
    "\n",
    "# Test with most deterministic settings\n",
    "print(f\"\\n🎯 TESTING MAXIMUM DETERMINISM:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Set all random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test with ultra-deterministic generation\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What color is the sky?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "test_prompt = processor.apply_chat_template(test_messages, add_generation_prompt=True)\n",
    "test_inputs = processor(text=test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"Input prompt: '{test_prompt}'\")\n",
    "print(f\"Input token IDs: {test_inputs['input_ids'].tolist()}\")\n",
    "\n",
    "# Generate with maximum determinism\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,              # Greedy decoding\n",
    "        temperature=1.0,              # Not used with do_sample=False, but set anyway\n",
    "        top_p=1.0,                    # Not used with do_sample=False\n",
    "        top_k=0,                      # Not used with do_sample=False\n",
    "        repetition_penalty=1.0,       # No repetition penalty\n",
    "        length_penalty=1.0,           # No length penalty\n",
    "        no_repeat_ngram_size=0,       # No n-gram blocking\n",
    "        pad_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,               # Use KV cache for consistency\n",
    "    )\n",
    "\n",
    "generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "output_tokens = outputs[0].tolist()\n",
    "\n",
    "print(f\"\\nGenerated tokens: {output_tokens}\")\n",
    "print(f\"Generated text: '{generated_text}'\")\n",
    "\n",
    "# Extract just the new tokens\n",
    "input_length = test_inputs['input_ids'].shape[1]\n",
    "new_tokens = output_tokens[input_length:]\n",
    "new_text = processor.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nNew tokens only: {new_tokens}\")\n",
    "print(f\"New text only: '{new_text}'\")\n",
    "\n",
    "print(f\"\\n🔧 POTENTIAL SOURCES OF DIFFERENCES:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. 🔢 NUMERICAL PRECISION:\n",
    "   - Python: Uses PyTorch's bf16/f32 precision\n",
    "   - Rust: Candle's precision might differ slightly\n",
    "   - Solution: Ensure exact same dtype operations\n",
    "\n",
    "2. 🧮 ATTENTION COMPUTATION:\n",
    "   - Softmax numerical stability differences\n",
    "   - Matrix multiplication order/accumulation\n",
    "   - Causal mask application timing\n",
    "   - Solution: Match exact computation order\n",
    "\n",
    "3. 🎭 LAYER NORMALIZATION:\n",
    "   - Epsilon values (1e-5 vs 1e-6)\n",
    "   - Computation order (x-mean)/std vs x/std - mean/std\n",
    "   - Solution: Use exact same epsilon and formula\n",
    "\n",
    "4. 🔄 TOKENIZATION:\n",
    "   - Preprocessing differences\n",
    "   - Special token handling\n",
    "   - Solution: Use identical tokenizer and special tokens\n",
    "\n",
    "5. 🖼️  IMAGE PREPROCESSING:\n",
    "   - Resize/normalization differences\n",
    "   - Padding strategies\n",
    "   - Patch splitting logic\n",
    "   - Solution: Match exact preprocessing pipeline\n",
    "\n",
    "6. 💾 WEIGHT LOADING:\n",
    "   - Tensor layout differences (row vs column major)\n",
    "   - Weight precision during loading\n",
    "   - Solution: Verify weight values match exactly\n",
    "\n",
    "7. 🎲 GENERATION LOGIC:\n",
    "   - Logit processing order\n",
    "   - Temperature application\n",
    "   - Token selection tie-breaking\n",
    "   - Solution: Match exact generation pipeline\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n✅ ACHIEVING EXACT PARITY:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"\"\"\n",
    "To get exact parity:\n",
    "\n",
    "1. 🔍 INTERMEDIATE OUTPUT COMPARISON:\n",
    "   - Print logits before softmax in both implementations\n",
    "   - Compare attention weights layer by layer\n",
    "   - Verify embeddings and hidden states match\n",
    "\n",
    "2. 🧪 UNIT TESTING:\n",
    "   - Test individual components (attention, MLP, etc.)\n",
    "   - Use simple inputs (zeros, ones, known patterns)\n",
    "   - Compare outputs at each layer\n",
    "\n",
    "3. 🎯 WEIGHT VERIFICATION:\n",
    "   - Extract weights from Python model\n",
    "   - Verify they load identically in Rust\n",
    "   - Check for any transpose/reshape differences\n",
    "\n",
    "4. 📊 PRECISION CONTROL:\n",
    "   - Use identical dtypes throughout pipeline\n",
    "   - Implement identical numerical algorithms\n",
    "   - Handle edge cases (div by zero, etc.) the same way\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15eeefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔧 PRACTICAL DEBUGGING FOR RUST PARITY\n",
      "================================================================================\n",
      "\n",
      "📋 STEP-BY-STEP DEBUGGING CHECKLIST:\n",
      "\n",
      "1. 🎯 FIRST: Test with simple text-only input\n",
      "   - Remove image processing from the equation\n",
      "   - Use: \"Hello world\" → check if outputs match\n",
      "   - This isolates text generation differences\n",
      "\n",
      "2. 🔍 INTERMEDIATE OUTPUTS:\n",
      "   Add debug prints in your Rust code at these points:\n",
      "\n",
      "   a) After tokenization:\n",
      "      println!(\"Input tokens: {:?}\", input_ids);\n",
      "\n",
      "   b) After embeddings:\n",
      "      println!(\"Embeddings mean: {}\", embeddings.mean());\n",
      "\n",
      "   c) After each transformer layer:\n",
      "      println!(\"Layer {} output mean: {}\", i, hidden_states.mean());\n",
      "\n",
      "   d) Before final logits:\n",
      "      println!(\"Pre-logits mean: {}\", pre_logits.mean());\n",
      "\n",
      "   e) Final logits for next token:\n",
      "      println!(\"Logits for position {}: {:?}\", pos, logits.slice());\n",
      "\n",
      "3. 🎲 GENERATION COMPARISON:\n",
      "   Run both implementations with identical settings:\n",
      "\n",
      "   Python:\n",
      "   ```python\n",
      "   outputs = model.generate(\n",
      "       input_ids=torch.tensor([[1, 2, 3]]),\n",
      "       max_new_tokens=1,\n",
      "       do_sample=False,\n",
      "       use_cache=False,  # Disable KV cache for debugging\n",
      "   )\n",
      "   ```\n",
      "\n",
      "   Rust:\n",
      "   ```rust\n",
      "   let outputs = model.generate(\n",
      "       &input_ids,\n",
      "       1, // max_new_tokens\n",
      "       false, // do_sample\n",
      "   );\n",
      "   ```\n",
      "\n",
      "4. 🔢 NUMERICAL DEBUGGING:\n",
      "   Check these specific values match:\n",
      "\n",
      "   - Embedding weights: model.embed_tokens.weight[0, :10]\n",
      "   - Layer norm epsilon: config.layer_norm_eps  \n",
      "   - Attention head dimensions\n",
      "   - RMSNorm vs LayerNorm implementation\n",
      "\n",
      "\n",
      "📊 KEY NUMERICAL VALUES FOR RUST COMPARISON:\n",
      "==================================================\n",
      "\n",
      "Embedding weights sample (first 5x5):\n",
      "tensor([[ 0.0251,  0.0129, -0.0165,  0.0005,  0.0151],\n",
      "        [ 0.0066,  0.0070, -0.0005,  0.0277,  0.0152],\n",
      "        [ 0.0018,  0.0137, -0.0021,  0.0306,  0.0189],\n",
      "        [-0.1377, -0.0515,  0.0271,  0.1416,  0.0162],\n",
      "        [-0.1050, -0.0640,  0.0044,  0.1064,  0.0161]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "\n",
      "Key config values:\n",
      "  hidden_size: 2048\n",
      "  num_attention_heads: 32\n",
      "  num_key_value_heads: 32\n",
      "  intermediate_size: 8192\n",
      "  rms_norm_eps: 1e-05\n",
      "  rope_theta: 273768.0\n",
      "  vocab_size: 49155\n",
      "\n",
      "🧪 MINIMAL TEST CASE:\n",
      "==============================\n",
      "Single token input: tensor([[1]], device='cuda:0')\n",
      "Top 5 next token logits: [16.0, 5.15625, 3.484375, 2.90625, 2.796875]\n",
      "Top 5 next token IDs: [49152, 44, 99, 330, 378]\n",
      "Top 5 next tokens: ['<fake_token_around_image>', '<', 's', ' A', ' The']\n",
      "\n",
      "💡 DOES IT MATTER?\n",
      "====================\n",
      "\n",
      "Small differences might be acceptable if:\n",
      "✅ Generated text is semantically similar\n",
      "✅ Differences are in low-probability tokens\n",
      "✅ Overall model performance is maintained\n",
      "\n",
      "Exact parity is needed if:\n",
      "❌ Building production systems requiring reproducibility\n",
      "❌ Comparing against benchmarks\n",
      "❌ Debugging model behavior\n",
      "❌ Academic research requiring exact reproduction\n",
      "\n",
      "For most applications, \"close enough\" is often sufficient!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔧 PRACTICAL DEBUGGING STEPS\n",
    "print(\"=\" * 80)\n",
    "print(\"🔧 PRACTICAL DEBUGGING FOR RUST PARITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "📋 STEP-BY-STEP DEBUGGING CHECKLIST:\n",
    "\n",
    "1. 🎯 FIRST: Test with simple text-only input\n",
    "   - Remove image processing from the equation\n",
    "   - Use: \"Hello world\" → check if outputs match\n",
    "   - This isolates text generation differences\n",
    "\n",
    "2. 🔍 INTERMEDIATE OUTPUTS:\n",
    "   Add debug prints in your Rust code at these points:\n",
    "   \n",
    "   a) After tokenization:\n",
    "      println!(\"Input tokens: {:?}\", input_ids);\n",
    "   \n",
    "   b) After embeddings:\n",
    "      println!(\"Embeddings mean: {}\", embeddings.mean());\n",
    "   \n",
    "   c) After each transformer layer:\n",
    "      println!(\"Layer {} output mean: {}\", i, hidden_states.mean());\n",
    "   \n",
    "   d) Before final logits:\n",
    "      println!(\"Pre-logits mean: {}\", pre_logits.mean());\n",
    "   \n",
    "   e) Final logits for next token:\n",
    "      println!(\"Logits for position {}: {:?}\", pos, logits.slice());\n",
    "\n",
    "3. 🎲 GENERATION COMPARISON:\n",
    "   Run both implementations with identical settings:\n",
    "   \n",
    "   Python:\n",
    "   ```python\n",
    "   outputs = model.generate(\n",
    "       input_ids=torch.tensor([[1, 2, 3]]),\n",
    "       max_new_tokens=1,\n",
    "       do_sample=False,\n",
    "       use_cache=False,  # Disable KV cache for debugging\n",
    "   )\n",
    "   ```\n",
    "   \n",
    "   Rust:\n",
    "   ```rust\n",
    "   let outputs = model.generate(\n",
    "       &input_ids,\n",
    "       1, // max_new_tokens\n",
    "       false, // do_sample\n",
    "   );\n",
    "   ```\n",
    "\n",
    "4. 🔢 NUMERICAL DEBUGGING:\n",
    "   Check these specific values match:\n",
    "   \n",
    "   - Embedding weights: model.embed_tokens.weight[0, :10]\n",
    "   - Layer norm epsilon: config.layer_norm_eps  \n",
    "   - Attention head dimensions\n",
    "   - RMSNorm vs LayerNorm implementation\n",
    "\"\"\")\n",
    "\n",
    "# Let's extract some key numerical values for comparison\n",
    "print(f\"\\n📊 KEY NUMERICAL VALUES FOR RUST COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get some embeddings to compare\n",
    "if hasattr(model, 'get_input_embeddings'):\n",
    "    embeddings = model.get_input_embeddings()\n",
    "    sample_embeddings = embeddings.weight[:5, :5]  # First 5 tokens, first 5 dims\n",
    "    print(f\"\\nEmbedding weights sample (first 5x5):\")\n",
    "    print(sample_embeddings)\n",
    "\n",
    "# Get model config values\n",
    "config = model.config\n",
    "if hasattr(config, 'text_config'):\n",
    "    text_config = config.text_config\n",
    "    print(f\"\\nKey config values:\")\n",
    "    print(f\"  hidden_size: {getattr(text_config, 'hidden_size', 'N/A')}\")\n",
    "    print(f\"  num_attention_heads: {getattr(text_config, 'num_attention_heads', 'N/A')}\")\n",
    "    print(f\"  num_key_value_heads: {getattr(text_config, 'num_key_value_heads', 'N/A')}\")\n",
    "    print(f\"  intermediate_size: {getattr(text_config, 'intermediate_size', 'N/A')}\")\n",
    "    print(f\"  rms_norm_eps: {getattr(text_config, 'rms_norm_eps', 'N/A')}\")\n",
    "    print(f\"  rope_theta: {getattr(text_config, 'rope_theta', 'N/A')}\")\n",
    "    print(f\"  vocab_size: {getattr(text_config, 'vocab_size', 'N/A')}\")\n",
    "\n",
    "# Test a minimal case\n",
    "print(f\"\\n🧪 MINIMAL TEST CASE:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Single token input\n",
    "single_token = torch.tensor([[1]]).to(\"cuda\")  # Usually BOS token\n",
    "print(f\"Single token input: {single_token}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get logits for next token\n",
    "    outputs = model(input_ids=single_token)\n",
    "    logits = outputs.logits[0, -1, :]  # Last position logits\n",
    "    \n",
    "    # Get top 5 most likely next tokens\n",
    "    top_logits, top_indices = torch.topk(logits, 5)\n",
    "    \n",
    "    print(f\"Top 5 next token logits: {top_logits.tolist()}\")\n",
    "    print(f\"Top 5 next token IDs: {top_indices.tolist()}\")\n",
    "    \n",
    "    # Convert to tokens\n",
    "    top_tokens = [processor.tokenizer.decode([idx]) for idx in top_indices]\n",
    "    print(f\"Top 5 next tokens: {top_tokens}\")\n",
    "\n",
    "print(f\"\\n💡 DOES IT MATTER?\")\n",
    "print(\"=\" * 20)\n",
    "print(\"\"\"\n",
    "Small differences might be acceptable if:\n",
    "✅ Generated text is semantically similar\n",
    "✅ Differences are in low-probability tokens\n",
    "✅ Overall model performance is maintained\n",
    "\n",
    "Exact parity is needed if:\n",
    "❌ Building production systems requiring reproducibility\n",
    "❌ Comparing against benchmarks\n",
    "❌ Debugging model behavior\n",
    "❌ Academic research requiring exact reproduction\n",
    "\n",
    "For most applications, \"close enough\" is often sufficient!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f0a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
