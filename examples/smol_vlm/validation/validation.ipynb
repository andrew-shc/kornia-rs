{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bac3792",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98ce996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "\n",
    "import torch, random, numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_all_seeds(9)\n",
    "\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05358e7",
   "metadata": {},
   "source": [
    "# Assign Prompts\n",
    "All codes are for PyTorch side.\n",
    "\n",
    "Run the following equivalent commands (select one) for the Kornia-rs side. (Activations are compared via the implicitly saved safetensor files.)\n",
    "```sh\n",
    "cargo run -p smol_vlm --features cuda -- -i .vscode/angela-porter-2021-jan-25.jpg -p \"Can you describe the image?\" --sample-length 500\n",
    "cargo run -p smol_vlm --features cuda -- -p \"A real-valued function f defined on the real line is called an even function if f(-t) = f(t) for each real number t. Prove that the set of even functions defined on the real line with the operations of addition and scalar multiplication defined in Example 3 is a vector space.\" --sample-length 200\n",
    "cargo run -p smol_vlm --features cuda -- -p \"Can you describe the image?\" --sample-length 500\n",
    "cargo run -p smol_vlm --features cuda -- -p \"What is life?\" --sample-length 500\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd567e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 size: (900, 900)\n"
     ]
    }
   ],
   "source": [
    "# image1 = load_image(\"https://media.istockphoto.com/id/485371557/photo/twilight-at-spirit-island.jpg?s=612x612&w=0&k=20&c=FSGliJ4EKFP70Yjpzso0HfRR4WwflC6GKfl4F3Hj7fk=\")\n",
    "# image2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\n",
    "# image1 = Image.open(\"../../../.vscode/fuji-mountain-in-autumn.jpg\")\n",
    "image1 = load_image(\"https://artwyrd.com/wp-content/uploads/2021/01/angela-porter-2021-jan-25.jpg\")\n",
    "print(f\"Image 1 size: {image1.size}\")\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\"type\": \"image\"},\n",
    "            # {\"type\": \"text\", \"text\": \"Can you describe the image?\"}\n",
    "            {\"type\": \"text\", \"text\": \"What is life?\"}\n",
    "            # {\"type\": \"text\", \"text\": \"A real-valued function f defined on the real line is called an even function if f(-t) = f(t) for each real number t. Prove that the set of even functions defined on the real line with the operations of addition and scalar multiplication defined in Example 3 is a vector space.\"}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afefb43",
   "metadata": {},
   "source": [
    "# Activation introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67fa3593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.idefics3.modeling_idefics3.Idefics3ForConditionalGeneration"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from activation_introspector import ActivationIntrospector\n",
    "\n",
    "# Initialize model directly on CUDA without Flash Attention\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # _attn_implementation=\"flash_attention_2\",  # Commented out Flash Attention\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "act_isp = ActivationIntrospector()\n",
    "layers = OrderedDict()\n",
    "\n",
    "layers[\"input_embeddings\"] = model.get_input_embeddings()\n",
    "for i in range(24):\n",
    "    layers[f\"input_layernorm_d{i}\"] = model.model.text_model.layers[i].input_layernorm\n",
    "    layers[f\"self_attn_d{i}\"]       = model.model.text_model.layers[i].self_attn\n",
    "    layers[f\"post_layernorm_d{i}\"]  = model.model.text_model.layers[i].post_attention_layernorm\n",
    "    #  layer[f\"mlp_d{i}\"]             = model.model.text_model.layers[i].mlp\n",
    "    layers[f\"mlp_gate_proj_d{i}\"]   = model.model.text_model.layers[i].mlp.gate_proj\n",
    "    layers[f\"mlp_up_proj_d{i}\"]     = model.model.text_model.layers[i].mlp.up_proj\n",
    "    layers[f\"mlp_down_proj_d{i}\"]   = model.model.text_model.layers[i].mlp.down_proj\n",
    "    layers[f\"mlp_act_fn_d{i}\"]      = model.model.text_model.layers[i].mlp.act_fn\n",
    "    layers[f\"layers_d{i}\"]          = model.model.text_model.layers[i]\n",
    "layers[\"logits\"] = model.get_output_embeddings()\n",
    "\n",
    "act_isp.set_introspector(layers)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18722a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Idefics3ForConditionalGeneration(\n",
       "  (model): Idefics3Model(\n",
       "    (vision_model): Idefics3VisionTransformer(\n",
       "      (embeddings): Idefics3VisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(729, 1152)\n",
       "      )\n",
       "      (encoder): Idefics3Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x Idefics3EncoderLayer(\n",
       "            (self_attn): Idefics3VisionAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Idefics3VisionMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (connector): Idefics3Connector(\n",
       "      (modality_projection): Idefics3SimpleMLP(\n",
       "        (proj): Linear(in_features=10368, out_features=2048, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (text_model): LlamaModel(\n",
       "      (embed_tokens): Embedding(49155, 2048, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49155, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8f203",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0599d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 11126,    42,  1812,   314,  1029,    47, 49154,   198,  9519,\n",
      "          9531,    42]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    1, 11126,    42,  1812,   314,  1029,    47, 49154,   198,  9519,\n",
       "         9531,    42,  5330,   314,   253,  1784,   284,  4013,  1909,   338,\n",
       "          553,   719,   260,  2138,   282, 11150,    28,  3097,    28,   284,\n",
       "         4955, 13926,   327,  5034,    30,  1814,   624,   768,  2987,  1106,\n",
       "           28,  1029,   314,   260,  9231,   338, 29792,  2242,  6896,   429,\n",
       "        47600,  3401,   284,  1603,    29, 23209,  2631,    30, 12532,  6896,\n",
       "          359,  5181,   282,  2063,    28,   639,    29, 46218, 10751,    28,\n",
       "        12166,    28,   284,  2426,   288,   260,  1357,    30,   198,   198,\n",
       "         5212,   253,  3097,  4939,    28,  1029,   314,  1129,  4355,   411,\n",
       "          260,  4313,   282,   253,  5748,  1297,    28,   527,   314,   260,\n",
       "         2987,  2723,   282,   511,  2242,  6896,    30, 22189,   359,  1135,\n",
       "          614,   282,  1461, 41266,   284,  3626,   338,   746,  1592,   288,\n",
       "         1477,  1895,  3691,   715,   347, 10751,    28,  2063,    28,   284,\n",
       "        12166,    30,   198,   198,  5212,   253, 11150,  4939,    28,  1029,\n",
       "          314,  1129,  2188,   351,  8650,    28,   639,    29, 18769,    28,\n",
       "          284,   260,  2470,   288,  1786,  4801,   284,  6118,    30,  2015,\n",
       "        15820,  7440,   338,  1029,   314,   253,  2116,   284,  9764, 10274,\n",
       "          338,   868,   325, 12637,   284,  6086,    28,   979,  1449,  2875,\n",
       "          338,  1029,   314,   253,  3086,   282,  2647,   284,  5759,   338,\n",
       "         5354,  1022,   288,  2112,    30,   198,   198,  5212,   253,  4955,\n",
       "         4939,    28,  1029,   314,  1129,  2269,   347,   253,  3444,  2258,\n",
       "        23619,    28,   639,    29, 20003,    28,   284,   260, 10122,  3491,\n",
       "          282,  8749,   253,  1215,   282,  8964,   351,   260,  9100,    30,\n",
       "         3027,  3320,  5174, 11111,   260,  2979,   282,  2242,   253, 34121,\n",
       "          284, 36321,  1029,    28,   284,   260,  3491,   282,  8749,   253,\n",
       "         1215,   282, 23619,   355,   304, 43925,    30,   198,   198,   788,\n",
       "         7127,    28,  1029,   314,   253,  1784,   284, 17333,  1909,   338,\n",
       "          553,   719,  8167,   284,  6182,   281,   800,   896,  1853,  2126,\n",
       "         1463,    30,   657,   314,   253,  4959,  2325,   282,  1205,  5387,\n",
       "           28,   284,   582,   338,   314,  1062,  9511,   284, 19985,  3924,\n",
       "          411,   701,   282,   511,  4520,   284, 10689,    30, 49154],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "print(inputs[\"input_ids\"])\n",
    "# Generate outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        # repition_penalty=1.1,  # Apply repeat penalty\n",
    "        output_scores=True,           # Return logits for each generated token\n",
    "        return_dict_in_generate=True, # Return detailed output object\n",
    "        do_sample=False,  # Use greedy decoding (highest logit)\n",
    "    )\n",
    "\n",
    "outputs.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f420759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User: What is life?\\nAssistant: Life is a complex and fascinating concept that has been the subject of philosophical, scientific, and spiritual inquiry for centuries. At its most basic level, life is the characteristic that distinguishes living organisms from inanimate objects and non-living matter. Living organisms are capable of growth, self-sustaining metabolism, reproduction, and response to the environment.\\n\\nFrom a scientific perspective, life is often defined by the presence of a biological cell, which is the basic unit of all living organisms. Cells are made up of various organelles and structures that work together to perform essential functions such as metabolism, growth, and reproduction.\\n\\nFrom a philosophical perspective, life is often associated with consciousness, self-awareness, and the ability to experience emotions and suffering. Some philosophers argue that life is a unique and precious gift that should be valued and protected, while others believe that life is a series of experiences and interactions that ultimately lead to death.\\n\\nFrom a spiritual perspective, life is often seen as a journey towards enlightenment, self-discovery, and the ultimate goal of achieving a state of union with the divine. Many religious traditions emphasize the importance of living a virtuous and purposeful life, and the goal of achieving a state of enlightenment or nirvana.\\n\\nIn summary, life is a complex and multifaceted concept that has been explored and understood in many different ways throughout history. It is a fundamental aspect of human existence, and one that is both celebrated and mourned by people of all cultures and religions.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40239c73",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e293022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at index 2: Python top 253, Rust top 260\n",
      "[  0] EMBEDS(MSE:0.00000000, MAE:0.00000000)  LOGITS(MSE:0.00166525, MAE:0.03089225) \n",
      "[PYTHON] Logits: [16.5, 14.0625, 12.25, 12.0625, 12.0625, 11.9375, 11.6875, 11.5, 11.375, 11.3125, 11.3125, 11.25, 11.0625, 10.9375, 10.9375], Tokens: [' Life', ' The', ' In', ' \"', ' To', ' It', ' This', ' A', ' Well', ' I', ' ', ' What', \" '\", ' According', ' There']\n",
      "         Tokens: [5330, 378, 533, 476, 1626, 657, 669, 330, 3929, 339, 216, 1812, 637, 3959, 1385]\n",
      "[RUST] Logits: [16.375, 14.0625, 12.25, 12.125, 12.0, 12.0, 11.75, 11.5, 11.4375, 11.375, 11.3125, 11.25, 11.0625, 11.0, 10.9375], Tokens: [' Life', ' The', ' In', ' To', ' \"', ' It', ' This', ' A', ' Well', ' What', ' I', ' ', \" '\", ' According', ' There']\n",
      "         Tokens: [5330, 378, 533, 1626, 476, 657, 669, 330, 3929, 1812, 339, 216, 637, 3959, 1385]\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000000, MAE:0.00000000)\n",
      "    [SELF ATTN]         (MSE:0.00000006, MAE:0.00004554)\n",
      "    [POST LAYERNORM]    (MSE:0.00000001, MAE:0.00003052)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000033, MAE:0.00030327)\n",
      "        [MLP/ACT FN]    (MSE:0.00000024, MAE:0.00027084)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000027, MAE:0.00028038)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00001353, MAE:0.00098419)\n",
      "[LAYER  0]        (MSE:0.00001609, MAE:0.00101471)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000052, MAE:0.00021267)\n",
      "    [SELF ATTN]         (MSE:0.00001055, MAE:0.00107574)\n",
      "    [POST LAYERNORM]    (MSE:0.00000032, MAE:0.00025558)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000462, MAE:0.00131226)\n",
      "        [MLP/ACT FN]    (MSE:0.00000091, MAE:0.00064850)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000270, MAE:0.00114441)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00325012, MAE:0.00297546)\n",
      "[LAYER  1]        (MSE:0.00305176, MAE:0.00334167)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000136, MAE:0.00050735)\n",
      "    [SELF ATTN]         (MSE:0.00000374, MAE:0.00112915)\n",
      "    [POST LAYERNORM]    (MSE:0.00000027, MAE:0.00031090)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000501, MAE:0.00149536)\n",
      "        [MLP/ACT FN]    (MSE:0.00000116, MAE:0.00071716)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000402, MAE:0.00134277)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00001729, MAE:0.00228882)\n",
      "[LAYER  2]        (MSE:0.00308228, MAE:0.00448608)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000186, MAE:0.00065613)\n",
      "    [SELF ATTN]         (MSE:0.00001401, MAE:0.00204468)\n",
      "    [POST LAYERNORM]    (MSE:0.00000041, MAE:0.00039101)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000805, MAE:0.00193024)\n",
      "        [MLP/ACT FN]    (MSE:0.00000188, MAE:0.00088882)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000656, MAE:0.00174713)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00002968, MAE:0.00326538)\n",
      "[LAYER  3]        (MSE:0.00332642, MAE:0.00607300)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000352, MAE:0.00080872)\n",
      "    [SELF ATTN]         (MSE:0.00001907, MAE:0.00277710)\n",
      "    [POST LAYERNORM]    (MSE:0.00000064, MAE:0.00050354)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001168, MAE:0.00233459)\n",
      "        [MLP/ACT FN]    (MSE:0.00000270, MAE:0.00106812)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000983, MAE:0.00213623)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00005150, MAE:0.00442505)\n",
      "[LAYER  4]        (MSE:0.00318909, MAE:0.00805664)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000331, MAE:0.00089645)\n",
      "    [SELF ATTN]         (MSE:0.00002551, MAE:0.00308228)\n",
      "    [POST LAYERNORM]    (MSE:0.00000086, MAE:0.00059509)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001693, MAE:0.00283813)\n",
      "        [MLP/ACT FN]    (MSE:0.00000387, MAE:0.00129700)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001413, MAE:0.00254822)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00007248, MAE:0.00579834)\n",
      "[LAYER  5]        (MSE:0.00335693, MAE:0.01007080)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000364, MAE:0.00106812)\n",
      "    [SELF ATTN]         (MSE:0.00002456, MAE:0.00331116)\n",
      "    [POST LAYERNORM]    (MSE:0.00000113, MAE:0.00068665)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001955, MAE:0.00300598)\n",
      "        [MLP/ACT FN]    (MSE:0.00000426, MAE:0.00132751)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001848, MAE:0.00296021)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00017834, MAE:0.00714111)\n",
      "[LAYER  6]        (MSE:0.00325012, MAE:0.01232910)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000474, MAE:0.00116730)\n",
      "    [SELF ATTN]         (MSE:0.00030327, MAE:0.00601196)\n",
      "    [POST LAYERNORM]    (MSE:0.00000373, MAE:0.00091934)\n",
      "        [MLP/GATE PROJ] (MSE:0.00003934, MAE:0.00415039)\n",
      "        [MLP/ACT FN]    (MSE:0.00001127, MAE:0.00176239)\n",
      "        [MLP/UP PROJ]   (MSE:0.00004506, MAE:0.00402832)\n",
      "        [MLP/DOWN PROJ] (MSE:0.68359375, MAE:0.02331543)\n",
      "[LAYER  7]        (MSE:0.71093750, MAE:0.02868652)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00003767, MAE:0.00219727)\n",
      "    [SELF ATTN]         (MSE:0.00331116, MAE:0.01116943)\n",
      "    [POST LAYERNORM]    (MSE:0.00000659, MAE:0.00145721)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010347, MAE:0.00665283)\n",
      "        [MLP/ACT FN]    (MSE:0.00002193, MAE:0.00253296)\n",
      "        [MLP/UP PROJ]   (MSE:0.00010109, MAE:0.00653076)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00181580, MAE:0.01928711)\n",
      "[LAYER  8]        (MSE:0.70703125, MAE:0.03881836)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002193, MAE:0.00213623)\n",
      "    [SELF ATTN]         (MSE:0.00028801, MAE:0.01190186)\n",
      "    [POST LAYERNORM]    (MSE:0.00000599, MAE:0.00159454)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010872, MAE:0.00689697)\n",
      "        [MLP/ACT FN]    (MSE:0.00001645, MAE:0.00239563)\n",
      "        [MLP/UP PROJ]   (MSE:0.00010109, MAE:0.00680542)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00077820, MAE:0.01892090)\n",
      "[LAYER  9]        (MSE:0.70703125, MAE:0.04077148)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002420, MAE:0.00239563)\n",
      "    [SELF ATTN]         (MSE:0.00037193, MAE:0.01269531)\n",
      "    [POST LAYERNORM]    (MSE:0.00000641, MAE:0.00167847)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010300, MAE:0.00686646)\n",
      "        [MLP/ACT FN]    (MSE:0.00001836, MAE:0.00250244)\n",
      "        [MLP/UP PROJ]   (MSE:0.00011158, MAE:0.00723267)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00092316, MAE:0.02075195)\n",
      "[LAYER 10]        (MSE:0.70703125, MAE:0.04370117)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001788, MAE:0.00233459)\n",
      "    [SELF ATTN]         (MSE:0.00026321, MAE:0.01092529)\n",
      "    [POST LAYERNORM]    (MSE:0.00000659, MAE:0.00171661)\n",
      "        [MLP/GATE PROJ] (MSE:0.00011015, MAE:0.00704956)\n",
      "        [MLP/ACT FN]    (MSE:0.00001931, MAE:0.00257874)\n",
      "        [MLP/UP PROJ]   (MSE:0.00011587, MAE:0.00738525)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00089645, MAE:0.02050781)\n",
      "[LAYER 11]        (MSE:0.70703125, MAE:0.04492188)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002289, MAE:0.00267029)\n",
      "    [SELF ATTN]         (MSE:0.00023937, MAE:0.01043701)\n",
      "    [POST LAYERNORM]    (MSE:0.00000730, MAE:0.00178528)\n",
      "        [MLP/GATE PROJ] (MSE:0.00012302, MAE:0.00732422)\n",
      "        [MLP/ACT FN]    (MSE:0.00002420, MAE:0.00285339)\n",
      "        [MLP/UP PROJ]   (MSE:0.00012684, MAE:0.00762939)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00104523, MAE:0.02160645)\n",
      "[LAYER 12]        (MSE:0.70703125, MAE:0.04785156)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002003, MAE:0.00256348)\n",
      "    [SELF ATTN]         (MSE:0.00020695, MAE:0.00988770)\n",
      "    [POST LAYERNORM]    (MSE:0.00000736, MAE:0.00178528)\n",
      "        [MLP/GATE PROJ] (MSE:0.00011778, MAE:0.00708008)\n",
      "        [MLP/ACT FN]    (MSE:0.00002539, MAE:0.00292969)\n",
      "        [MLP/UP PROJ]   (MSE:0.00013065, MAE:0.00759888)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00112915, MAE:0.02246094)\n",
      "[LAYER 13]        (MSE:0.70703125, MAE:0.05004883)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001693, MAE:0.00241089)\n",
      "    [SELF ATTN]         (MSE:0.00022030, MAE:0.01007080)\n",
      "    [POST LAYERNORM]    (MSE:0.00000700, MAE:0.00174713)\n",
      "        [MLP/GATE PROJ] (MSE:0.00011349, MAE:0.00698853)\n",
      "        [MLP/ACT FN]    (MSE:0.00002730, MAE:0.00306702)\n",
      "        [MLP/UP PROJ]   (MSE:0.00012589, MAE:0.00747681)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00124359, MAE:0.02343750)\n",
      "[LAYER 14]        (MSE:0.70703125, MAE:0.05224609)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002193, MAE:0.00286865)\n",
      "    [SELF ATTN]         (MSE:0.00027466, MAE:0.01086426)\n",
      "    [POST LAYERNORM]    (MSE:0.00000721, MAE:0.00177002)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010872, MAE:0.00683594)\n",
      "        [MLP/ACT FN]    (MSE:0.00002909, MAE:0.00314331)\n",
      "        [MLP/UP PROJ]   (MSE:0.00012779, MAE:0.00747681)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00158691, MAE:0.02673340)\n",
      "[LAYER 15]        (MSE:0.70703125, MAE:0.05737305)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002122, MAE:0.00270081)\n",
      "    [SELF ATTN]         (MSE:0.00024605, MAE:0.01043701)\n",
      "    [POST LAYERNORM]    (MSE:0.00000823, MAE:0.00186920)\n",
      "        [MLP/GATE PROJ] (MSE:0.00012684, MAE:0.00735474)\n",
      "        [MLP/ACT FN]    (MSE:0.00003457, MAE:0.00338745)\n",
      "        [MLP/UP PROJ]   (MSE:0.00015068, MAE:0.00811768)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00231934, MAE:0.03149414)\n",
      "[LAYER 16]        (MSE:0.71093750, MAE:0.06494141)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001991, MAE:0.00263977)\n",
      "    [SELF ATTN]         (MSE:0.00042534, MAE:0.01452637)\n",
      "    [POST LAYERNORM]    (MSE:0.00000930, MAE:0.00198364)\n",
      "        [MLP/GATE PROJ] (MSE:0.00014591, MAE:0.00787354)\n",
      "        [MLP/ACT FN]    (MSE:0.00004339, MAE:0.00376892)\n",
      "        [MLP/UP PROJ]   (MSE:0.00016785, MAE:0.00854492)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00317383, MAE:0.03735352)\n",
      "[LAYER 17]        (MSE:0.71484375, MAE:0.07666016)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002098, MAE:0.00274658)\n",
      "    [SELF ATTN]         (MSE:0.00040436, MAE:0.01385498)\n",
      "    [POST LAYERNORM]    (MSE:0.00001192, MAE:0.00222778)\n",
      "        [MLP/GATE PROJ] (MSE:0.00018311, MAE:0.00872803)\n",
      "        [MLP/ACT FN]    (MSE:0.00005317, MAE:0.00408936)\n",
      "        [MLP/UP PROJ]   (MSE:0.00021839, MAE:0.00964355)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00515747, MAE:0.04663086)\n",
      "[LAYER 18]        (MSE:0.71875000, MAE:0.09033203)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002480, MAE:0.00299072)\n",
      "    [SELF ATTN]         (MSE:0.00064087, MAE:0.01635742)\n",
      "    [POST LAYERNORM]    (MSE:0.00001538, MAE:0.00245667)\n",
      "        [MLP/GATE PROJ] (MSE:0.00023079, MAE:0.00958252)\n",
      "        [MLP/ACT FN]    (MSE:0.00006962, MAE:0.00457764)\n",
      "        [MLP/UP PROJ]   (MSE:0.00028229, MAE:0.01062012)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00738525, MAE:0.05493164)\n",
      "[LAYER 19]        (MSE:0.72656250, MAE:0.10839844)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00003040, MAE:0.00315857)\n",
      "    [SELF ATTN]         (MSE:0.00065231, MAE:0.01672363)\n",
      "    [POST LAYERNORM]    (MSE:0.00001788, MAE:0.00262451)\n",
      "        [MLP/GATE PROJ] (MSE:0.00025940, MAE:0.01007080)\n",
      "        [MLP/ACT FN]    (MSE:0.00007963, MAE:0.00488281)\n",
      "        [MLP/UP PROJ]   (MSE:0.00032997, MAE:0.01141357)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00927734, MAE:0.05981445)\n",
      "[LAYER 20]        (MSE:0.73437500, MAE:0.12792969)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00003767, MAE:0.00355530)\n",
      "    [SELF ATTN]         (MSE:0.00415039, MAE:0.03295898)\n",
      "    [POST LAYERNORM]    (MSE:0.00002408, MAE:0.00292969)\n",
      "        [MLP/GATE PROJ] (MSE:0.00035095, MAE:0.01123047)\n",
      "        [MLP/ACT FN]    (MSE:0.00009966, MAE:0.00524902)\n",
      "        [MLP/UP PROJ]   (MSE:0.00044441, MAE:0.01275635)\n",
      "        [MLP/DOWN PROJ] (MSE:0.01409912, MAE:0.06884766)\n",
      "[LAYER 21]        (MSE:0.75390625, MAE:0.15429688)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00004339, MAE:0.00384521)\n",
      "    [SELF ATTN]         (MSE:0.00346375, MAE:0.03662109)\n",
      "    [POST LAYERNORM]    (MSE:0.00002444, MAE:0.00302124)\n",
      "        [MLP/GATE PROJ] (MSE:0.00035858, MAE:0.01165771)\n",
      "        [MLP/ACT FN]    (MSE:0.00010729, MAE:0.00561523)\n",
      "        [MLP/UP PROJ]   (MSE:0.00046349, MAE:0.01318359)\n",
      "        [MLP/DOWN PROJ] (MSE:0.73437500, MAE:0.09521484)\n",
      "[LAYER 22]        (MSE:0.09619141, MAE:0.18261719)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00004911, MAE:0.00433350)\n",
      "    [SELF ATTN]         (MSE:0.01513672, MAE:0.06640625)\n",
      "    [POST LAYERNORM]    (MSE:0.00003672, MAE:0.00373840)\n",
      "        [MLP/GATE PROJ] (MSE:0.00056458, MAE:0.01586914)\n",
      "        [MLP/ACT FN]    (MSE:0.00022602, MAE:0.00811768)\n",
      "        [MLP/UP PROJ]   (MSE:0.00065231, MAE:0.01745605)\n",
      "        [MLP/DOWN PROJ] (MSE:0.13769531, MAE:0.20996094)\n",
      "[LAYER 23]        (MSE:0.21582031, MAE:0.29687500)\n",
      "[  1] EMBEDS(MSE:0.00000000, MAE:0.00000000)  LOGITS(MSE:0.00527189, MAE:0.06046125) \n",
      "[PYTHON] Logits: [21.5, 18.5, 18.125, 17.5, 16.25, 15.9375, 15.6875, 15.25, 15.125, 15.0625, 14.8125, 14.75, 14.625, 14.625, 14.5625], Tokens: [' is', ',', ' can', ' refers', ' in', ' has', ' means', ' as', ' could', ' itself', ' (', ' comes', ' appears', ' on', ' consists']\n",
      "         Tokens: [314, 28, 416, 4271, 281, 553, 1530, 347, 856, 2581, 365, 2216, 4541, 335, 5956]\n",
      "[RUST] Logits: [21.5, 18.5, 18.25, 17.375, 16.25, 15.9375, 15.6875, 15.25, 15.125, 15.0, 14.8125, 14.75, 14.75, 14.625, 14.5], Tokens: [' is', ',', ' can', ' refers', ' in', ' has', ' means', ' as', ' could', ' itself', ' (', ' comes', ' on', ' appears', ' isn']\n",
      "         Tokens: [314, 28, 416, 4271, 281, 553, 1530, 347, 856, 2581, 365, 2216, 335, 4541, 3247]\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000000, MAE:0.00000000)\n",
      "    [SELF ATTN]         (MSE:0.00000001, MAE:0.00005674)\n",
      "    [POST LAYERNORM]    (MSE:0.00000001, MAE:0.00003052)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000029, MAE:0.00030136)\n",
      "        [MLP/ACT FN]    (MSE:0.00000022, MAE:0.00026894)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000025, MAE:0.00028038)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00000153, MAE:0.00070953)\n",
      "[LAYER  0]        (MSE:0.00000314, MAE:0.00073242)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000092, MAE:0.00034714)\n",
      "    [SELF ATTN]         (MSE:0.00000235, MAE:0.00102234)\n",
      "    [POST LAYERNORM]    (MSE:0.00000022, MAE:0.00031471)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000399, MAE:0.00146484)\n",
      "        [MLP/ACT FN]    (MSE:0.00000094, MAE:0.00069809)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000308, MAE:0.00133514)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00001007, MAE:0.00234985)\n",
      "[LAYER  1]        (MSE:0.00002062, MAE:0.00265503)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000381, MAE:0.00081635)\n",
      "    [SELF ATTN]         (MSE:0.00000507, MAE:0.00157928)\n",
      "    [POST LAYERNORM]    (MSE:0.00000040, MAE:0.00046349)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000781, MAE:0.00210571)\n",
      "        [MLP/ACT FN]    (MSE:0.00000174, MAE:0.00097275)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000611, MAE:0.00189972)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00001705, MAE:0.00323486)\n",
      "[LAYER  2]        (MSE:0.00003576, MAE:0.00430298)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000183, MAE:0.00091553)\n",
      "    [SELF ATTN]         (MSE:0.00001436, MAE:0.00292969)\n",
      "    [POST LAYERNORM]    (MSE:0.00000056, MAE:0.00056076)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001073, MAE:0.00241089)\n",
      "        [MLP/ACT FN]    (MSE:0.00000221, MAE:0.00106812)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000894, MAE:0.00228882)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00003648, MAE:0.00460815)\n",
      "[LAYER  3]        (MSE:0.00010109, MAE:0.00701904)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000396, MAE:0.00113678)\n",
      "    [SELF ATTN]         (MSE:0.00005126, MAE:0.00561523)\n",
      "    [POST LAYERNORM]    (MSE:0.00000106, MAE:0.00079727)\n",
      "        [MLP/GATE PROJ] (MSE:0.00002265, MAE:0.00366211)\n",
      "        [MLP/ACT FN]    (MSE:0.00000456, MAE:0.00151062)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001752, MAE:0.00323486)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00008535, MAE:0.00738525)\n",
      "[LAYER  4]        (MSE:0.00021458, MAE:0.01049805)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000528, MAE:0.00143433)\n",
      "    [SELF ATTN]         (MSE:0.00002587, MAE:0.00402832)\n",
      "    [POST LAYERNORM]    (MSE:0.00000153, MAE:0.00095749)\n",
      "        [MLP/GATE PROJ] (MSE:0.00002789, MAE:0.00411987)\n",
      "        [MLP/ACT FN]    (MSE:0.00000593, MAE:0.00178528)\n",
      "        [MLP/UP PROJ]   (MSE:0.00002456, MAE:0.00389099)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00012875, MAE:0.00891113)\n",
      "[LAYER  5]        (MSE:0.00033188, MAE:0.01324463)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000858, MAE:0.00171661)\n",
      "    [SELF ATTN]         (MSE:0.00004458, MAE:0.00521851)\n",
      "    [POST LAYERNORM]    (MSE:0.00000198, MAE:0.00106812)\n",
      "        [MLP/GATE PROJ] (MSE:0.00003242, MAE:0.00442505)\n",
      "        [MLP/ACT FN]    (MSE:0.00000724, MAE:0.00191498)\n",
      "        [MLP/UP PROJ]   (MSE:0.00003147, MAE:0.00436401)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00021362, MAE:0.01147461)\n",
      "[LAYER  6]        (MSE:0.00061798, MAE:0.01696777)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000942, MAE:0.00161743)\n",
      "    [SELF ATTN]         (MSE:0.00002992, MAE:0.00405884)\n",
      "    [POST LAYERNORM]    (MSE:0.00000228, MAE:0.00116730)\n",
      "        [MLP/GATE PROJ] (MSE:0.00003672, MAE:0.00473022)\n",
      "        [MLP/ACT FN]    (MSE:0.00000817, MAE:0.00202942)\n",
      "        [MLP/UP PROJ]   (MSE:0.00003886, MAE:0.00491333)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00029755, MAE:0.01336670)\n",
      "[LAYER  7]        (MSE:0.00095749, MAE:0.02075195)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000614, MAE:0.00173187)\n",
      "    [SELF ATTN]         (MSE:0.00025177, MAE:0.00662231)\n",
      "    [POST LAYERNORM]    (MSE:0.00000356, MAE:0.00148010)\n",
      "        [MLP/GATE PROJ] (MSE:0.00005722, MAE:0.00585938)\n",
      "        [MLP/ACT FN]    (MSE:0.00001085, MAE:0.00224304)\n",
      "        [MLP/UP PROJ]   (MSE:0.00005913, MAE:0.00604248)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00042725, MAE:0.01635742)\n",
      "[LAYER  8]        (MSE:0.00153351, MAE:0.02453613)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001013, MAE:0.00210571)\n",
      "    [SELF ATTN]         (MSE:0.00010729, MAE:0.00787354)\n",
      "    [POST LAYERNORM]    (MSE:0.00000423, MAE:0.00160217)\n",
      "        [MLP/GATE PROJ] (MSE:0.00006628, MAE:0.00628662)\n",
      "        [MLP/ACT FN]    (MSE:0.00001127, MAE:0.00231934)\n",
      "        [MLP/UP PROJ]   (MSE:0.00007248, MAE:0.00665283)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00061798, MAE:0.01965332)\n",
      "[LAYER  9]        (MSE:0.00224304, MAE:0.02758789)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001609, MAE:0.00231934)\n",
      "    [SELF ATTN]         (MSE:0.00026894, MAE:0.01281738)\n",
      "    [POST LAYERNORM]    (MSE:0.00000551, MAE:0.00180817)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008631, MAE:0.00714111)\n",
      "        [MLP/ACT FN]    (MSE:0.00001526, MAE:0.00262451)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009394, MAE:0.00756836)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00084686, MAE:0.02160645)\n",
      "[LAYER 10]        (MSE:0.00181580, MAE:0.02929688)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000942, MAE:0.00222778)\n",
      "    [SELF ATTN]         (MSE:0.00022316, MAE:0.01171875)\n",
      "    [POST LAYERNORM]    (MSE:0.00000477, MAE:0.00169373)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007963, MAE:0.00680542)\n",
      "        [MLP/ACT FN]    (MSE:0.00001508, MAE:0.00265503)\n",
      "        [MLP/UP PROJ]   (MSE:0.00008249, MAE:0.00714111)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00091553, MAE:0.02185059)\n",
      "[LAYER 11]        (MSE:0.00171661, MAE:0.03173828)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001097, MAE:0.00248718)\n",
      "    [SELF ATTN]         (MSE:0.00032043, MAE:0.01324463)\n",
      "    [POST LAYERNORM]    (MSE:0.00000435, MAE:0.00161743)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007105, MAE:0.00646973)\n",
      "        [MLP/ACT FN]    (MSE:0.00001413, MAE:0.00250244)\n",
      "        [MLP/UP PROJ]   (MSE:0.00007629, MAE:0.00683594)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00079727, MAE:0.02197266)\n",
      "[LAYER 12]        (MSE:0.00191498, MAE:0.03295898)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000760, MAE:0.00207520)\n",
      "    [SELF ATTN]         (MSE:0.00013256, MAE:0.00909424)\n",
      "    [POST LAYERNORM]    (MSE:0.00000367, MAE:0.00149536)\n",
      "        [MLP/GATE PROJ] (MSE:0.00005841, MAE:0.00579834)\n",
      "        [MLP/ACT FN]    (MSE:0.00001210, MAE:0.00238037)\n",
      "        [MLP/UP PROJ]   (MSE:0.00006342, MAE:0.00619507)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00067139, MAE:0.02014160)\n",
      "[LAYER 13]        (MSE:0.00239563, MAE:0.03320312)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000718, MAE:0.00199890)\n",
      "    [SELF ATTN]         (MSE:0.00024605, MAE:0.01159668)\n",
      "    [POST LAYERNORM]    (MSE:0.00000352, MAE:0.00144958)\n",
      "        [MLP/GATE PROJ] (MSE:0.00005651, MAE:0.00573730)\n",
      "        [MLP/ACT FN]    (MSE:0.00001377, MAE:0.00251770)\n",
      "        [MLP/UP PROJ]   (MSE:0.00006151, MAE:0.00616455)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00056076, MAE:0.01892090)\n",
      "[LAYER 14]        (MSE:0.00236511, MAE:0.03369141)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000966, MAE:0.00225830)\n",
      "    [SELF ATTN]         (MSE:0.00014591, MAE:0.00939941)\n",
      "    [POST LAYERNORM]    (MSE:0.00000340, MAE:0.00141144)\n",
      "        [MLP/GATE PROJ] (MSE:0.00005579, MAE:0.00567627)\n",
      "        [MLP/ACT FN]    (MSE:0.00001448, MAE:0.00254822)\n",
      "        [MLP/UP PROJ]   (MSE:0.00005937, MAE:0.00604248)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00059509, MAE:0.01904297)\n",
      "[LAYER 15]        (MSE:0.00227356, MAE:0.03588867)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000840, MAE:0.00201416)\n",
      "    [SELF ATTN]         (MSE:0.00022316, MAE:0.01177979)\n",
      "    [POST LAYERNORM]    (MSE:0.00000364, MAE:0.00147247)\n",
      "        [MLP/GATE PROJ] (MSE:0.00005770, MAE:0.00576782)\n",
      "        [MLP/ACT FN]    (MSE:0.00001514, MAE:0.00263977)\n",
      "        [MLP/UP PROJ]   (MSE:0.00006533, MAE:0.00631714)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00088120, MAE:0.02331543)\n",
      "[LAYER 16]        (MSE:0.00352478, MAE:0.04296875)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000793, MAE:0.00201416)\n",
      "    [SELF ATTN]         (MSE:0.00038338, MAE:0.01507568)\n",
      "    [POST LAYERNORM]    (MSE:0.00000399, MAE:0.00156403)\n",
      "        [MLP/GATE PROJ] (MSE:0.00006628, MAE:0.00616455)\n",
      "        [MLP/ACT FN]    (MSE:0.00001824, MAE:0.00279236)\n",
      "        [MLP/UP PROJ]   (MSE:0.00007248, MAE:0.00668335)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00137329, MAE:0.02929688)\n",
      "[LAYER 17]        (MSE:0.00515747, MAE:0.05151367)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000846, MAE:0.00212097)\n",
      "    [SELF ATTN]         (MSE:0.00056839, MAE:0.01745605)\n",
      "    [POST LAYERNORM]    (MSE:0.00000516, MAE:0.00174713)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008297, MAE:0.00698853)\n",
      "        [MLP/ACT FN]    (MSE:0.00002658, MAE:0.00323486)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009298, MAE:0.00750732)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00238037, MAE:0.03784180)\n",
      "[LAYER 18]        (MSE:0.00668335, MAE:0.06201172)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000924, MAE:0.00212097)\n",
      "    [SELF ATTN]         (MSE:0.00052261, MAE:0.01745605)\n",
      "    [POST LAYERNORM]    (MSE:0.00000513, MAE:0.00174713)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008059, MAE:0.00689697)\n",
      "        [MLP/ACT FN]    (MSE:0.00002670, MAE:0.00329590)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009823, MAE:0.00772095)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00311279, MAE:0.04394531)\n",
      "[LAYER 19]        (MSE:0.01232910, MAE:0.07861328)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001001, MAE:0.00231934)\n",
      "    [SELF ATTN]         (MSE:0.00053024, MAE:0.01770020)\n",
      "    [POST LAYERNORM]    (MSE:0.00000584, MAE:0.00186157)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008535, MAE:0.00717163)\n",
      "        [MLP/ACT FN]    (MSE:0.00003338, MAE:0.00381470)\n",
      "        [MLP/UP PROJ]   (MSE:0.00010681, MAE:0.00811768)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00379944, MAE:0.04858398)\n",
      "[LAYER 20]        (MSE:0.01721191, MAE:0.09619141)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001341, MAE:0.00263977)\n",
      "    [SELF ATTN]         (MSE:0.00063705, MAE:0.02014160)\n",
      "    [POST LAYERNORM]    (MSE:0.00000733, MAE:0.00204468)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010681, MAE:0.00805664)\n",
      "        [MLP/ACT FN]    (MSE:0.00004172, MAE:0.00430298)\n",
      "        [MLP/UP PROJ]   (MSE:0.00013351, MAE:0.00909424)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00442505, MAE:0.05273438)\n",
      "[LAYER 21]        (MSE:0.02270508, MAE:0.11279297)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001442, MAE:0.00280762)\n",
      "    [SELF ATTN]         (MSE:0.00160217, MAE:0.03149414)\n",
      "    [POST LAYERNORM]    (MSE:0.00000787, MAE:0.00213623)\n",
      "        [MLP/GATE PROJ] (MSE:0.00011063, MAE:0.00811768)\n",
      "        [MLP/ACT FN]    (MSE:0.00004411, MAE:0.00445557)\n",
      "        [MLP/UP PROJ]   (MSE:0.00013447, MAE:0.00903320)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00466919, MAE:0.05371094)\n",
      "[LAYER 22]        (MSE:0.02954102, MAE:0.13476562)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001609, MAE:0.00300598)\n",
      "    [SELF ATTN]         (MSE:0.00370789, MAE:0.04248047)\n",
      "    [POST LAYERNORM]    (MSE:0.00000742, MAE:0.00209045)\n",
      "        [MLP/GATE PROJ] (MSE:0.00011683, MAE:0.00817871)\n",
      "        [MLP/ACT FN]    (MSE:0.00004077, MAE:0.00402832)\n",
      "        [MLP/UP PROJ]   (MSE:0.00014019, MAE:0.00903320)\n",
      "        [MLP/DOWN PROJ] (MSE:0.01977539, MAE:0.09521484)\n",
      "[LAYER 23]        (MSE:0.05859375, MAE:0.18066406)\n",
      "[  2] EMBEDS(MSE:0.00000000, MAE:0.00000000)  LOGITS(MSE:0.00422011, MAE:0.05384992) \n",
      "[PYTHON] Logits: [19.125, 19.125, 17.375, 17.125, 16.25, 16.125, 15.875, 15.375, 15.375, 15.3125, 15.3125, 15.25, 15.125, 14.9375, 14.8125], Tokens: [' a', ' the', ' something', ' an', ' defined', ' what', ' not', ' one', ' all', ' anything', ' everything', ' more', ' when', ' about', ' any']\n",
      "         Tokens: [253, 260, 1488, 354, 4355, 732, 441, 582, 511, 3534, 3117, 540, 645, 563, 750]\n",
      "[RUST] Logits: [19.125, 19.125, 17.375, 17.125, 16.25, 16.125, 15.875, 15.4375, 15.375, 15.3125, 15.3125, 15.25, 15.125, 14.9375, 14.8125], Tokens: [' the', ' a', ' something', ' an', ' defined', ' what', ' not', ' one', ' all', ' anything', ' more', ' everything', ' when', ' about', ' any']\n",
      "         Tokens: [260, 253, 1488, 354, 4355, 732, 441, 582, 511, 3534, 540, 3117, 645, 563, 750]\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000000, MAE:0.00000000)\n",
      "    [SELF ATTN]         (MSE:0.00000001, MAE:0.00004625)\n",
      "    [POST LAYERNORM]    (MSE:0.00000000, MAE:0.00002527)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000015, MAE:0.00022697)\n",
      "        [MLP/ACT FN]    (MSE:0.00000008, MAE:0.00018215)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000012, MAE:0.00020504)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00000073, MAE:0.00056839)\n",
      "[LAYER  0]        (MSE:0.00000083, MAE:0.00057602)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000025, MAE:0.00028419)\n",
      "    [SELF ATTN]         (MSE:0.00000244, MAE:0.00100708)\n",
      "    [POST LAYERNORM]    (MSE:0.00000016, MAE:0.00029182)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000387, MAE:0.00137329)\n",
      "        [MLP/ACT FN]    (MSE:0.00000084, MAE:0.00067520)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000267, MAE:0.00125122)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00000569, MAE:0.00173950)\n",
      "[LAYER  1]        (MSE:0.00000948, MAE:0.00199890)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000268, MAE:0.00083160)\n",
      "    [SELF ATTN]         (MSE:0.00001597, MAE:0.00314331)\n",
      "    [POST LAYERNORM]    (MSE:0.00000073, MAE:0.00064468)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001371, MAE:0.00285339)\n",
      "        [MLP/ACT FN]    (MSE:0.00000319, MAE:0.00131226)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001144, MAE:0.00263977)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00004077, MAE:0.00500488)\n",
      "[LAYER  2]        (MSE:0.00006533, MAE:0.00613403)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000498, MAE:0.00139618)\n",
      "    [SELF ATTN]         (MSE:0.00000757, MAE:0.00212097)\n",
      "    [POST LAYERNORM]    (MSE:0.00000101, MAE:0.00076294)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001752, MAE:0.00325012)\n",
      "        [MLP/ACT FN]    (MSE:0.00000384, MAE:0.00145721)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001621, MAE:0.00318909)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00005317, MAE:0.00567627)\n",
      "[LAYER  3]        (MSE:0.00012493, MAE:0.00805664)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001007, MAE:0.00151825)\n",
      "    [SELF ATTN]         (MSE:0.00001979, MAE:0.00338745)\n",
      "    [POST LAYERNORM]    (MSE:0.00000128, MAE:0.00086594)\n",
      "        [MLP/GATE PROJ] (MSE:0.00002420, MAE:0.00382996)\n",
      "        [MLP/ACT FN]    (MSE:0.00000402, MAE:0.00150299)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001991, MAE:0.00349426)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00006819, MAE:0.00646973)\n",
      "[LAYER  4]        (MSE:0.00016689, MAE:0.00903320)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000528, MAE:0.00148773)\n",
      "    [SELF ATTN]         (MSE:0.00002599, MAE:0.00405884)\n",
      "    [POST LAYERNORM]    (MSE:0.00000141, MAE:0.00091553)\n",
      "        [MLP/GATE PROJ] (MSE:0.00002313, MAE:0.00375366)\n",
      "        [MLP/ACT FN]    (MSE:0.00000462, MAE:0.00157928)\n",
      "        [MLP/UP PROJ]   (MSE:0.00002265, MAE:0.00373840)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00009203, MAE:0.00759888)\n",
      "[LAYER  5]        (MSE:0.00022316, MAE:0.01080322)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000709, MAE:0.00158691)\n",
      "    [SELF ATTN]         (MSE:0.00012398, MAE:0.00860596)\n",
      "    [POST LAYERNORM]    (MSE:0.00000177, MAE:0.00102997)\n",
      "        [MLP/GATE PROJ] (MSE:0.00003266, MAE:0.00442505)\n",
      "        [MLP/ACT FN]    (MSE:0.00000650, MAE:0.00183868)\n",
      "        [MLP/UP PROJ]   (MSE:0.00002837, MAE:0.00418091)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00017738, MAE:0.01037598)\n",
      "[LAYER  6]        (MSE:0.00047302, MAE:0.01599121)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000763, MAE:0.00173950)\n",
      "    [SELF ATTN]         (MSE:0.00006390, MAE:0.00576782)\n",
      "    [POST LAYERNORM]    (MSE:0.00000244, MAE:0.00119781)\n",
      "        [MLP/GATE PROJ] (MSE:0.00003791, MAE:0.00479126)\n",
      "        [MLP/ACT FN]    (MSE:0.00000718, MAE:0.00193024)\n",
      "        [MLP/UP PROJ]   (MSE:0.00003910, MAE:0.00491333)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00019836, MAE:0.01092529)\n",
      "[LAYER  7]        (MSE:0.00059891, MAE:0.01757812)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000620, MAE:0.00183105)\n",
      "    [SELF ATTN]         (MSE:0.00013924, MAE:0.00909424)\n",
      "    [POST LAYERNORM]    (MSE:0.00000270, MAE:0.00127411)\n",
      "        [MLP/GATE PROJ] (MSE:0.00004745, MAE:0.00527954)\n",
      "        [MLP/ACT FN]    (MSE:0.00000659, MAE:0.00178528)\n",
      "        [MLP/UP PROJ]   (MSE:0.00004506, MAE:0.00524902)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00028419, MAE:0.01318359)\n",
      "[LAYER  8]        (MSE:0.00079346, MAE:0.01855469)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000739, MAE:0.00186157)\n",
      "    [SELF ATTN]         (MSE:0.00027847, MAE:0.01318359)\n",
      "    [POST LAYERNORM]    (MSE:0.00000393, MAE:0.00153351)\n",
      "        [MLP/GATE PROJ] (MSE:0.00006199, MAE:0.00610352)\n",
      "        [MLP/ACT FN]    (MSE:0.00001067, MAE:0.00219727)\n",
      "        [MLP/UP PROJ]   (MSE:0.00006580, MAE:0.00637817)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00049591, MAE:0.01770020)\n",
      "[LAYER  9]        (MSE:0.00113678, MAE:0.02416992)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000948, MAE:0.00228882)\n",
      "    [SELF ATTN]         (MSE:0.00026131, MAE:0.01263428)\n",
      "    [POST LAYERNORM]    (MSE:0.00000477, MAE:0.00169373)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008011, MAE:0.00689697)\n",
      "        [MLP/ACT FN]    (MSE:0.00001425, MAE:0.00248718)\n",
      "        [MLP/UP PROJ]   (MSE:0.00008249, MAE:0.00714111)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00058746, MAE:0.01940918)\n",
      "[LAYER 10]        (MSE:0.00143433, MAE:0.02648926)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001180, MAE:0.00239563)\n",
      "    [SELF ATTN]         (MSE:0.00044060, MAE:0.01660156)\n",
      "    [POST LAYERNORM]    (MSE:0.00000504, MAE:0.00176239)\n",
      "        [MLP/GATE PROJ] (MSE:0.00009155, MAE:0.00729370)\n",
      "        [MLP/ACT FN]    (MSE:0.00001585, MAE:0.00257874)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009775, MAE:0.00769043)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00076675, MAE:0.02197266)\n",
      "[LAYER 11]        (MSE:0.00148010, MAE:0.02954102)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001144, MAE:0.00263977)\n",
      "    [SELF ATTN]         (MSE:0.00069046, MAE:0.01977539)\n",
      "    [POST LAYERNORM]    (MSE:0.00000530, MAE:0.00180054)\n",
      "        [MLP/GATE PROJ] (MSE:0.00009346, MAE:0.00732422)\n",
      "        [MLP/ACT FN]    (MSE:0.00001669, MAE:0.00263977)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009966, MAE:0.00781250)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00113678, MAE:0.02624512)\n",
      "[LAYER 12]        (MSE:0.00173950, MAE:0.03222656)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001085, MAE:0.00251770)\n",
      "    [SELF ATTN]         (MSE:0.00039291, MAE:0.01562500)\n",
      "    [POST LAYERNORM]    (MSE:0.00000560, MAE:0.00183868)\n",
      "        [MLP/GATE PROJ] (MSE:0.00009108, MAE:0.00729370)\n",
      "        [MLP/ACT FN]    (MSE:0.00001991, MAE:0.00291443)\n",
      "        [MLP/UP PROJ]   (MSE:0.00010538, MAE:0.00799561)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00118256, MAE:0.02709961)\n",
      "[LAYER 13]        (MSE:0.00225830, MAE:0.03662109)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001115, MAE:0.00251770)\n",
      "    [SELF ATTN]         (MSE:0.00049973, MAE:0.01696777)\n",
      "    [POST LAYERNORM]    (MSE:0.00000492, MAE:0.00170898)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008917, MAE:0.00720215)\n",
      "        [MLP/ACT FN]    (MSE:0.00001848, MAE:0.00289917)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009823, MAE:0.00778198)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00109863, MAE:0.02587891)\n",
      "[LAYER 14]        (MSE:0.00230408, MAE:0.03735352)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001335, MAE:0.00274658)\n",
      "    [SELF ATTN]         (MSE:0.00033951, MAE:0.01440430)\n",
      "    [POST LAYERNORM]    (MSE:0.00000459, MAE:0.00167847)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007105, MAE:0.00646973)\n",
      "        [MLP/ACT FN]    (MSE:0.00001895, MAE:0.00294495)\n",
      "        [MLP/UP PROJ]   (MSE:0.00008631, MAE:0.00729370)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00110626, MAE:0.02648926)\n",
      "[LAYER 15]        (MSE:0.00271606, MAE:0.03979492)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001216, MAE:0.00238037)\n",
      "    [SELF ATTN]         (MSE:0.00028229, MAE:0.01300049)\n",
      "    [POST LAYERNORM]    (MSE:0.00000435, MAE:0.00159454)\n",
      "        [MLP/GATE PROJ] (MSE:0.00006962, MAE:0.00640869)\n",
      "        [MLP/ACT FN]    (MSE:0.00002050, MAE:0.00300598)\n",
      "        [MLP/UP PROJ]   (MSE:0.00008535, MAE:0.00723267)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00164032, MAE:0.03198242)\n",
      "[LAYER 16]        (MSE:0.00473022, MAE:0.05004883)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001037, MAE:0.00230408)\n",
      "    [SELF ATTN]         (MSE:0.00048065, MAE:0.01684570)\n",
      "    [POST LAYERNORM]    (MSE:0.00000495, MAE:0.00170135)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008011, MAE:0.00686646)\n",
      "        [MLP/ACT FN]    (MSE:0.00002944, MAE:0.00355530)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009680, MAE:0.00769043)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00233459, MAE:0.03735352)\n",
      "[LAYER 17]        (MSE:0.00680542, MAE:0.06079102)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000972, MAE:0.00228882)\n",
      "    [SELF ATTN]         (MSE:0.00077438, MAE:0.02001953)\n",
      "    [POST LAYERNORM]    (MSE:0.00000629, MAE:0.00186157)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010157, MAE:0.00775146)\n",
      "        [MLP/ACT FN]    (MSE:0.00003982, MAE:0.00399780)\n",
      "        [MLP/UP PROJ]   (MSE:0.00011778, MAE:0.00842285)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00405884, MAE:0.05053711)\n",
      "[LAYER 18]        (MSE:0.01171875, MAE:0.07714844)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001085, MAE:0.00245667)\n",
      "    [SELF ATTN]         (MSE:0.00035477, MAE:0.01489258)\n",
      "    [POST LAYERNORM]    (MSE:0.00000691, MAE:0.00199890)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010777, MAE:0.00799561)\n",
      "        [MLP/ACT FN]    (MSE:0.00004315, MAE:0.00418091)\n",
      "        [MLP/UP PROJ]   (MSE:0.00013256, MAE:0.00897217)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00549316, MAE:0.05859375)\n",
      "[LAYER 19]        (MSE:0.01782227, MAE:0.09814453)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001383, MAE:0.00253296)\n",
      "    [SELF ATTN]         (MSE:0.00037956, MAE:0.01470947)\n",
      "    [POST LAYERNORM]    (MSE:0.00000745, MAE:0.00209045)\n",
      "        [MLP/GATE PROJ] (MSE:0.00011206, MAE:0.00817871)\n",
      "        [MLP/ACT FN]    (MSE:0.00005007, MAE:0.00460815)\n",
      "        [MLP/UP PROJ]   (MSE:0.00014019, MAE:0.00921631)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00610352, MAE:0.06152344)\n",
      "[LAYER 20]        (MSE:0.02502441, MAE:0.12060547)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001562, MAE:0.00289917)\n",
      "    [SELF ATTN]         (MSE:0.00080109, MAE:0.02075195)\n",
      "    [POST LAYERNORM]    (MSE:0.00000906, MAE:0.00231934)\n",
      "        [MLP/GATE PROJ] (MSE:0.00013828, MAE:0.00921631)\n",
      "        [MLP/ACT FN]    (MSE:0.00006199, MAE:0.00521851)\n",
      "        [MLP/UP PROJ]   (MSE:0.00018120, MAE:0.01068115)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00823975, MAE:0.07031250)\n",
      "[LAYER 21]        (MSE:0.03906250, MAE:0.14160156)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001693, MAE:0.00311279)\n",
      "    [SELF ATTN]         (MSE:0.00106049, MAE:0.02441406)\n",
      "    [POST LAYERNORM]    (MSE:0.00001007, MAE:0.00242615)\n",
      "        [MLP/GATE PROJ] (MSE:0.00015926, MAE:0.00964355)\n",
      "        [MLP/ACT FN]    (MSE:0.00006437, MAE:0.00524902)\n",
      "        [MLP/UP PROJ]   (MSE:0.00019741, MAE:0.01098633)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00939941, MAE:0.07617188)\n",
      "[LAYER 22]        (MSE:0.05004883, MAE:0.16406250)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002110, MAE:0.00354004)\n",
      "    [SELF ATTN]         (MSE:0.01831055, MAE:0.05908203)\n",
      "    [POST LAYERNORM]    (MSE:0.00001353, MAE:0.00254822)\n",
      "        [MLP/GATE PROJ] (MSE:0.00019836, MAE:0.01043701)\n",
      "        [MLP/ACT FN]    (MSE:0.00008488, MAE:0.00534058)\n",
      "        [MLP/UP PROJ]   (MSE:0.00022411, MAE:0.01147461)\n",
      "        [MLP/DOWN PROJ] (MSE:0.05297852, MAE:0.12011719)\n",
      "[LAYER 23]        (MSE:0.10986328, MAE:0.21875000)\n",
      "[  3] EMBEDS(MSE:0.00000000, MAE:0.00000000)  LOGITS(MSE:0.00246530, MAE:0.03939308) \n",
      "[PYTHON] Logits: [16.75, 16.625, 16.25, 16.0, 15.9375, 15.875, 15.625, 15.625, 15.5625, 15.5, 15.1875, 15.1875, 15.125, 15.0625, 14.9375], Tokens: [' complex', ' state', ' phenomenon', ' process', ' word', ' journey', ' very', ' fundamental', ' continuous', ' wonderful', ' term', ' precious', ' concept', ' unique', ' beautiful']\n",
      "         Tokens: [1784, 1215, 7613, 980, 2229, 3444, 1035, 4959, 6860, 8264, 2115, 9764, 1909, 2116, 3953]\n",
      "[RUST] Logits: [16.875, 16.5, 16.25, 16.125, 15.875, 15.875, 15.6875, 15.625, 15.5625, 15.5625, 15.25, 15.1875, 15.0625, 15.0625, 14.9375], Tokens: [' complex', ' state', ' phenomenon', ' process', ' journey', ' word', ' fundamental', ' very', ' wonderful', ' continuous', ' precious', ' term', ' unique', ' concept', ' beautiful']\n",
      "         Tokens: [1784, 1215, 7613, 980, 3444, 2229, 4959, 1035, 8264, 6860, 9764, 2115, 2116, 1909, 3953]\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000000, MAE:0.00000000)\n",
      "    [SELF ATTN]         (MSE:0.00000002, MAE:0.00005627)\n",
      "    [POST LAYERNORM]    (MSE:0.00000001, MAE:0.00002789)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000028, MAE:0.00029945)\n",
      "        [MLP/ACT FN]    (MSE:0.00000011, MAE:0.00021458)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000018, MAE:0.00026321)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00000062, MAE:0.00051117)\n",
      "[LAYER  0]        (MSE:0.00000085, MAE:0.00052643)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000033, MAE:0.00026894)\n",
      "    [SELF ATTN]         (MSE:0.00000206, MAE:0.00108337)\n",
      "    [POST LAYERNORM]    (MSE:0.00000017, MAE:0.00028992)\n",
      "        [MLP/GATE PROJ] (MSE:0.00000435, MAE:0.00146484)\n",
      "        [MLP/ACT FN]    (MSE:0.00000086, MAE:0.00069809)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000276, MAE:0.00127411)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00000456, MAE:0.00160217)\n",
      "[LAYER  1]        (MSE:0.00000858, MAE:0.00184631)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000104, MAE:0.00068283)\n",
      "    [SELF ATTN]         (MSE:0.00001270, MAE:0.00277710)\n",
      "    [POST LAYERNORM]    (MSE:0.00000052, MAE:0.00054550)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001001, MAE:0.00242615)\n",
      "        [MLP/ACT FN]    (MSE:0.00000221, MAE:0.00110626)\n",
      "        [MLP/UP PROJ]   (MSE:0.00000840, MAE:0.00224304)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00002587, MAE:0.00393677)\n",
      "[LAYER  2]        (MSE:0.00004244, MAE:0.00485229)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000258, MAE:0.00110626)\n",
      "    [SELF ATTN]         (MSE:0.00001812, MAE:0.00335693)\n",
      "    [POST LAYERNORM]    (MSE:0.00000075, MAE:0.00067139)\n",
      "        [MLP/GATE PROJ] (MSE:0.00001466, MAE:0.00299072)\n",
      "        [MLP/ACT FN]    (MSE:0.00000325, MAE:0.00134277)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001228, MAE:0.00271606)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00003910, MAE:0.00494385)\n",
      "[LAYER  3]        (MSE:0.00009012, MAE:0.00723267)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000387, MAE:0.00133514)\n",
      "    [SELF ATTN]         (MSE:0.00002241, MAE:0.00369263)\n",
      "    [POST LAYERNORM]    (MSE:0.00000119, MAE:0.00085449)\n",
      "        [MLP/GATE PROJ] (MSE:0.00002122, MAE:0.00352478)\n",
      "        [MLP/ACT FN]    (MSE:0.00000429, MAE:0.00151062)\n",
      "        [MLP/UP PROJ]   (MSE:0.00001860, MAE:0.00337219)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00006390, MAE:0.00637817)\n",
      "[LAYER  4]        (MSE:0.00014687, MAE:0.00939941)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000399, MAE:0.00143433)\n",
      "    [SELF ATTN]         (MSE:0.00003767, MAE:0.00485229)\n",
      "    [POST LAYERNORM]    (MSE:0.00000154, MAE:0.00097275)\n",
      "        [MLP/GATE PROJ] (MSE:0.00002587, MAE:0.00396729)\n",
      "        [MLP/ACT FN]    (MSE:0.00000530, MAE:0.00170135)\n",
      "        [MLP/UP PROJ]   (MSE:0.00002468, MAE:0.00389099)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00009775, MAE:0.00787354)\n",
      "[LAYER  5]        (MSE:0.00023651, MAE:0.01165771)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000572, MAE:0.00167084)\n",
      "    [SELF ATTN]         (MSE:0.00010967, MAE:0.00778198)\n",
      "    [POST LAYERNORM]    (MSE:0.00000210, MAE:0.00111389)\n",
      "        [MLP/GATE PROJ] (MSE:0.00003719, MAE:0.00473022)\n",
      "        [MLP/ACT FN]    (MSE:0.00000775, MAE:0.00201416)\n",
      "        [MLP/UP PROJ]   (MSE:0.00003409, MAE:0.00460815)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00017071, MAE:0.01019287)\n",
      "[LAYER  6]        (MSE:0.00039291, MAE:0.01544189)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000581, MAE:0.00181580)\n",
      "    [SELF ATTN]         (MSE:0.00009537, MAE:0.00759888)\n",
      "    [POST LAYERNORM]    (MSE:0.00000277, MAE:0.00128174)\n",
      "        [MLP/GATE PROJ] (MSE:0.00004649, MAE:0.00531006)\n",
      "        [MLP/ACT FN]    (MSE:0.00000966, MAE:0.00222778)\n",
      "        [MLP/UP PROJ]   (MSE:0.00004792, MAE:0.00543213)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00026512, MAE:0.01293945)\n",
      "[LAYER  7]        (MSE:0.00061417, MAE:0.01928711)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000662, MAE:0.00196838)\n",
      "    [SELF ATTN]         (MSE:0.00007391, MAE:0.00674438)\n",
      "    [POST LAYERNORM]    (MSE:0.00000349, MAE:0.00145721)\n",
      "        [MLP/GATE PROJ] (MSE:0.00005603, MAE:0.00579834)\n",
      "        [MLP/ACT FN]    (MSE:0.00000936, MAE:0.00213623)\n",
      "        [MLP/UP PROJ]   (MSE:0.00005722, MAE:0.00595093)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00034142, MAE:0.01470947)\n",
      "[LAYER  8]        (MSE:0.00073624, MAE:0.02111816)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000665, MAE:0.00196838)\n",
      "    [SELF ATTN]         (MSE:0.00024605, MAE:0.01214600)\n",
      "    [POST LAYERNORM]    (MSE:0.00000405, MAE:0.00155640)\n",
      "        [MLP/GATE PROJ] (MSE:0.00006771, MAE:0.00637817)\n",
      "        [MLP/ACT FN]    (MSE:0.00001198, MAE:0.00238037)\n",
      "        [MLP/UP PROJ]   (MSE:0.00006866, MAE:0.00653076)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00049591, MAE:0.01770020)\n",
      "[LAYER  9]        (MSE:0.00099182, MAE:0.02453613)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000924, MAE:0.00228882)\n",
      "    [SELF ATTN]         (MSE:0.00017071, MAE:0.01007080)\n",
      "    [POST LAYERNORM]    (MSE:0.00000450, MAE:0.00164795)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007677, MAE:0.00677490)\n",
      "        [MLP/ACT FN]    (MSE:0.00001341, MAE:0.00245667)\n",
      "        [MLP/UP PROJ]   (MSE:0.00007772, MAE:0.00692749)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00057983, MAE:0.01892090)\n",
      "[LAYER 10]        (MSE:0.00120544, MAE:0.02661133)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00000995, MAE:0.00231934)\n",
      "    [SELF ATTN]         (MSE:0.00032997, MAE:0.01373291)\n",
      "    [POST LAYERNORM]    (MSE:0.00000498, MAE:0.00173950)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008392, MAE:0.00704956)\n",
      "        [MLP/ACT FN]    (MSE:0.00001508, MAE:0.00260925)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009441, MAE:0.00759888)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00070572, MAE:0.02124023)\n",
      "[LAYER 11]        (MSE:0.00144958, MAE:0.02905273)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001252, MAE:0.00267029)\n",
      "    [SELF ATTN]         (MSE:0.00028801, MAE:0.01336670)\n",
      "    [POST LAYERNORM]    (MSE:0.00000492, MAE:0.00173187)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007915, MAE:0.00683594)\n",
      "        [MLP/ACT FN]    (MSE:0.00001466, MAE:0.00254822)\n",
      "        [MLP/UP PROJ]   (MSE:0.00008535, MAE:0.00726318)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00067520, MAE:0.02062988)\n",
      "[LAYER 12]        (MSE:0.00160980, MAE:0.03051758)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001192, MAE:0.00247192)\n",
      "    [SELF ATTN]         (MSE:0.00033760, MAE:0.01422119)\n",
      "    [POST LAYERNORM]    (MSE:0.00000525, MAE:0.00174713)\n",
      "        [MLP/GATE PROJ] (MSE:0.00009918, MAE:0.00762939)\n",
      "        [MLP/ACT FN]    (MSE:0.00002003, MAE:0.00294495)\n",
      "        [MLP/UP PROJ]   (MSE:0.00010157, MAE:0.00781250)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00115204, MAE:0.02612305)\n",
      "[LAYER 13]        (MSE:0.00205994, MAE:0.03540039)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001252, MAE:0.00257874)\n",
      "    [SELF ATTN]         (MSE:0.00029755, MAE:0.01342773)\n",
      "    [POST LAYERNORM]    (MSE:0.00000489, MAE:0.00170898)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008297, MAE:0.00683594)\n",
      "        [MLP/ACT FN]    (MSE:0.00001800, MAE:0.00280762)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009203, MAE:0.00741577)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00098419, MAE:0.02416992)\n",
      "[LAYER 14]        (MSE:0.00245667, MAE:0.03662109)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001991, MAE:0.00286865)\n",
      "    [SELF ATTN]         (MSE:0.00025177, MAE:0.01245117)\n",
      "    [POST LAYERNORM]    (MSE:0.00000483, MAE:0.00169373)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007772, MAE:0.00680542)\n",
      "        [MLP/ACT FN]    (MSE:0.00002110, MAE:0.00308228)\n",
      "        [MLP/UP PROJ]   (MSE:0.00008821, MAE:0.00735474)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00123596, MAE:0.02758789)\n",
      "[LAYER 15]        (MSE:0.00306702, MAE:0.04150391)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001341, MAE:0.00248718)\n",
      "    [SELF ATTN]         (MSE:0.00029182, MAE:0.01306152)\n",
      "    [POST LAYERNORM]    (MSE:0.00000486, MAE:0.00167847)\n",
      "        [MLP/GATE PROJ] (MSE:0.00007820, MAE:0.00677490)\n",
      "        [MLP/ACT FN]    (MSE:0.00002122, MAE:0.00306702)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009012, MAE:0.00741577)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00156403, MAE:0.03076172)\n",
      "[LAYER 16]        (MSE:0.00488281, MAE:0.04907227)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001383, MAE:0.00239563)\n",
      "    [SELF ATTN]         (MSE:0.00055695, MAE:0.01794434)\n",
      "    [POST LAYERNORM]    (MSE:0.00000545, MAE:0.00176239)\n",
      "        [MLP/GATE PROJ] (MSE:0.00008583, MAE:0.00698853)\n",
      "        [MLP/ACT FN]    (MSE:0.00002706, MAE:0.00340271)\n",
      "        [MLP/UP PROJ]   (MSE:0.00009966, MAE:0.00781250)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00238037, MAE:0.03857422)\n",
      "[LAYER 17]        (MSE:0.00717163, MAE:0.06127930)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001115, MAE:0.00236511)\n",
      "    [SELF ATTN]         (MSE:0.00035667, MAE:0.01440430)\n",
      "    [POST LAYERNORM]    (MSE:0.00000602, MAE:0.00183868)\n",
      "        [MLP/GATE PROJ] (MSE:0.00009632, MAE:0.00750732)\n",
      "        [MLP/ACT FN]    (MSE:0.00003362, MAE:0.00375366)\n",
      "        [MLP/UP PROJ]   (MSE:0.00011778, MAE:0.00842285)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00369263, MAE:0.04809570)\n",
      "[LAYER 18]        (MSE:0.01037598, MAE:0.07421875)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001162, MAE:0.00239563)\n",
      "    [SELF ATTN]         (MSE:0.00028992, MAE:0.01269531)\n",
      "    [POST LAYERNORM]    (MSE:0.00000653, MAE:0.00191498)\n",
      "        [MLP/GATE PROJ] (MSE:0.00010824, MAE:0.00799561)\n",
      "        [MLP/ACT FN]    (MSE:0.00004125, MAE:0.00408936)\n",
      "        [MLP/UP PROJ]   (MSE:0.00012684, MAE:0.00872803)\n",
      "        [MLP/DOWN PROJ] (MSE:0.00567627, MAE:0.05908203)\n",
      "[LAYER 19]        (MSE:0.02380371, MAE:0.09716797)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001562, MAE:0.00260925)\n",
      "    [SELF ATTN]         (MSE:0.00074387, MAE:0.01989746)\n",
      "    [POST LAYERNORM]    (MSE:0.00000864, MAE:0.00221252)\n",
      "        [MLP/GATE PROJ] (MSE:0.00013542, MAE:0.00903320)\n",
      "        [MLP/ACT FN]    (MSE:0.00006628, MAE:0.00512695)\n",
      "        [MLP/UP PROJ]   (MSE:0.00017357, MAE:0.01031494)\n",
      "        [MLP/DOWN PROJ] (MSE:0.01055908, MAE:0.08154297)\n",
      "[LAYER 20]        (MSE:0.04516602, MAE:0.12890625)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002241, MAE:0.00323486)\n",
      "    [SELF ATTN]         (MSE:0.00144958, MAE:0.02490234)\n",
      "    [POST LAYERNORM]    (MSE:0.00000930, MAE:0.00230408)\n",
      "        [MLP/GATE PROJ] (MSE:0.00014305, MAE:0.00933838)\n",
      "        [MLP/ACT FN]    (MSE:0.00006485, MAE:0.00534058)\n",
      "        [MLP/UP PROJ]   (MSE:0.00018883, MAE:0.01080322)\n",
      "        [MLP/DOWN PROJ] (MSE:0.01007080, MAE:0.07910156)\n",
      "[LAYER 21]        (MSE:0.04638672, MAE:0.15234375)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00001907, MAE:0.00309753)\n",
      "    [SELF ATTN]         (MSE:0.00176239, MAE:0.03295898)\n",
      "    [POST LAYERNORM]    (MSE:0.00000983, MAE:0.00231934)\n",
      "        [MLP/GATE PROJ] (MSE:0.00015545, MAE:0.00952148)\n",
      "        [MLP/ACT FN]    (MSE:0.00006866, MAE:0.00546265)\n",
      "        [MLP/UP PROJ]   (MSE:0.00019455, MAE:0.01086426)\n",
      "        [MLP/DOWN PROJ] (MSE:0.01013184, MAE:0.07910156)\n",
      "[LAYER 22]        (MSE:0.05371094, MAE:0.17675781)\n",
      "    [INPUT LAYERNORM]   (MSE:0.00002456, MAE:0.00346375)\n",
      "    [SELF ATTN]         (MSE:0.00708008, MAE:0.06298828)\n",
      "    [POST LAYERNORM]    (MSE:0.00001234, MAE:0.00257874)\n",
      "        [MLP/GATE PROJ] (MSE:0.00028992, MAE:0.01263428)\n",
      "        [MLP/ACT FN]    (MSE:0.00011969, MAE:0.00677490)\n",
      "        [MLP/UP PROJ]   (MSE:0.00031281, MAE:0.01373291)\n",
      "        [MLP/DOWN PROJ] (MSE:0.05932617, MAE:0.15136719)\n",
      "[LAYER 23]        (MSE:0.11914062, MAE:0.24707031)\n"
     ]
    }
   ],
   "source": [
    "calc_err = lambda rust, python: (\n",
    "    torch.nn.functional.mse_loss(rust.squeeze(), python.squeeze()).item(),\n",
    "    torch.nn.functional.l1_loss(rust.squeeze(), python.squeeze()).item()\n",
    ")\n",
    "top_err = lambda rust, python: (rust.argsort(descending=True)[0].item(), python.squeeze().argsort(descending=True)[0].item())\n",
    "topK_logits = lambda rust, python: (rust.argsort(descending=True)[:15], python.squeeze().argsort(descending=True)[:15])\n",
    "\n",
    "err_fmt = lambda mse, mae: f\"(MSE:{mse:.8f}, MAE:{mae:.8f})\"\n",
    "\n",
    "with safe_open(\"../validation_data/rust_introspection.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "    logits_output = act_isp.compare_rust_activations(\n",
    "        range(200), f, top_err, subset=[\"logits\"]\n",
    "    )\n",
    "    for i in range(200):\n",
    "        top_logits_rust = logits_output[f\"logits_i{i}\"][0]\n",
    "        top_logits_python = logits_output[f\"logits_i{i}\"][1]\n",
    "        if top_logits_python != top_logits_rust:\n",
    "            print(f\"Mismatch at index {i}: Python top {top_logits_python}, Rust top {top_logits_rust}\")\n",
    "            ind_div = i\n",
    "            break\n",
    "\n",
    "    topK_logits_output = act_isp.compare_rust_activations(range(200), f, topK_logits, subset=[\"logits\"])\n",
    "    all_output = act_isp.compare_rust_activations(range(200), f, calc_err)\n",
    "    for i in range(max(0,ind_div-3), ind_div+2):\n",
    "        print(f\"[{i:>3}] EMBEDS{err_fmt(*all_output[f\"input_embeddings_i{i}\"])}  LOGITS{err_fmt(*all_output[f\"logits_i{i}\"])} \")\n",
    "\n",
    "        top_logits_rust = topK_logits_output[f\"logits_i{i}\"][0]\n",
    "        top_logits_python = topK_logits_output[f\"logits_i{i}\"][1]\n",
    "        rust_logits = f.get_tensor(f\"logits_i{i}\").squeeze().cpu()\n",
    "        python_logits = act_isp.activations[f\"logits_i{i}\"].squeeze().cpu()\n",
    "        print(f\"[PYTHON] Logits: {[python_logits[i].item() for i in top_logits_python]}, Tokens: {[processor.tokenizer.decode([i]) for i in top_logits_python]}\")\n",
    "        print(f\"         Tokens:\", [i.item() for i in top_logits_python])\n",
    "        print(f\"[RUST] Logits: {[rust_logits[i].item() for i in top_logits_rust]}, Tokens: {[processor.tokenizer.decode([i]) for i in top_logits_rust]}\")\n",
    "        print(f\"         Tokens:\", [i.item() for i in top_logits_rust])\n",
    "\n",
    "\n",
    "        # for layer_d in [0]:\n",
    "        for layer_d in range(24):\n",
    "            layer_fmt = lambda layer: err_fmt(*all_output[f\"{layer}_d{layer_d}_i{i}\"])\n",
    "\n",
    "            print(f\"    [INPUT LAYERNORM]   {layer_fmt(\"input_layernorm\")}\")\n",
    "            print(f\"    [SELF ATTN]         {layer_fmt(\"self_attn\")}\")\n",
    "            print(f\"    [POST LAYERNORM]    {layer_fmt(\"post_layernorm\")}\")\n",
    "            # print(f\"    [MLP]              {layer_fmt(\"mlp\")}\")\n",
    "            print(f\"        [MLP/GATE PROJ] {layer_fmt(\"mlp_gate_proj\")}\")\n",
    "            print(f\"        [MLP/ACT FN]    {layer_fmt(\"mlp_act_fn\")}\")\n",
    "            print(f\"        [MLP/UP PROJ]   {layer_fmt(\"mlp_up_proj\")}\")\n",
    "            print(f\"        [MLP/DOWN PROJ] {layer_fmt(\"mlp_down_proj\")}\")\n",
    "            print(f\"[LAYER {layer_d:>2}]        {layer_fmt(\"layers\")}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
